{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e99e204e",
   "metadata": {},
   "source": [
    "# Implementation of 2D Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c95c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cbae035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "## Downloading Dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape) # (60000, 28, 28)\n",
    "print(X_test.shape) # (10000, 28, 28)\n",
    "print(X_train[0].dtype) # uint8\n",
    "#print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cb0c92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQAUlEQVR4nO3dfaxUdX7H8fdH1LYiitSKlEVZWItVY9kNYuuSVeOyKtHo9WGztCY0EDFdabRpSS39YzUt1taHZonGBaMuNFt0EzUg3S0aULFrQ7wiKsKyWsOu6C2swSsPPhX49o85uFe885vLzJkH7u/zSiZzZr7nzPk68cM5Z84596eIwMwGvyPa3YCZtYbDbpYJh90sEw67WSYcdrNMOOxmmXDYD3OStkj65gDnDUlfqXM9dS9rncFht6aT9KykjyXtLh6b291Tjhx2a5U5EXFs8ZjQ7mZy5LAPIpImS/pvSb2SeiTdK+nog2abJuktSe9JulPSEX2Wnylpk6T3Ja2UdGqL/xOsiRz2wWUf8FfAicCfABcB3z1oni5gEvA14ApgJoCkK4F5wFXA7wHPA0sHslJJt0haUWO2fyr+gfmZpAsG8rlWsojw4zB+AFuAb1ap3Qw80ed1AJf0ef1dYFUx/VNgVp/aEcCHwKl9lv1KnT2eCwwDfguYAewCxrf7u8vt4S37ICLpDyStkPS/knYCt1PZyvf1dp/pXwK/X0yfCny/OAToBXYAAkY32ldErI2IXRHxSUQsBn4GTGv0c+3QOOyDy/3Az4HTIuI4KrvlOmieMX2mTwHeLabfBm6IiOF9Hr8TES80oc/opy9rMod9cBkG7AR2Szod+It+5pkr6QRJY4CbgEeL938A/J2kMwEkHS/p2kYbkjRc0sWSflvSkZL+DPgGsLLRz7ZD47APLn8D/CmVY+IH+E2Q+1oGvASsB/4DeBAgIp4A/hl4pDgE2ABcOpCVSpon6adVykcB/wj8GngP+EvgyojwufYWU/EDipkNct6ym2XCYTfLhMNulgmH3SwTR7ZyZZL8a6BZk0VEv9cwNLRll3SJpM2S3pR0SyOfZWbNVfepN0lDgF8AU4GtwIvA9IjYmFjGW3azJmvGln0y8GZEvBURnwKPULmLysw6UCNhH83nb6rYSj83TUiaLalbUncD6zKzBjXyA11/uwpf2E2PiEXAIvBuvFk7NbJl38rn76D6Er+5g8rMOkwjYX8ROE3Sl4s/ffQdYHk5bZlZ2erejY+IvZLmULlVcQjwUES8XlpnZlaqlt715mN2s+ZrykU1Znb4cNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulom6h2y2w8OQIUOS9eOPP76p658zZ07V2jHHHJNcdsKECcn6jTfemKzfddddVWvTp09PLvvxxx8n63fccUeyfttttyXr7dBQ2CVtAXYB+4C9ETGpjKbMrHxlbNkvjIj3SvgcM2siH7ObZaLRsAfwlKSXJM3ubwZJsyV1S+pucF1m1oBGd+O/HhHvSjoJeFrSzyNiTd8ZImIRsAhAUjS4PjOrU0Nb9oh4t3jeDjwBTC6jKTMrX91hlzRU0rAD08C3gA1lNWZm5WpkN34k8ISkA5/z7xHxn6V0NciccsopyfrRRx+drJ933nnJ+pQpU6rWhg8fnlz26quvTtbbaevWrcn6ggULkvWurq6qtV27diWXfeWVV5L15557LlnvRHWHPSLeAv6oxF7MrIl86s0sEw67WSYcdrNMOOxmmXDYzTKhiNZd1DZYr6CbOHFisr569epkvdm3mXaq/fv3J+szZ85M1nfv3l33unt6epL1999/P1nfvHlz3etutohQf+97y26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcLn2UswYsSIZH3t2rXJ+rhx48psp1S1eu/t7U3WL7zwwqq1Tz/9NLlsrtcfNMrn2c0y57CbZcJhN8uEw26WCYfdLBMOu1kmHHazTHjI5hLs2LEjWZ87d26yftlllyXrL7/8crJe608qp6xfvz5Znzp1arK+Z8+eZP3MM8+sWrvpppuSy1q5vGU3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLh+9k7wHHHHZes1xpeeOHChVVrs2bNSi573XXXJetLly5N1q3z1H0/u6SHJG2XtKHPeyMkPS3pjeL5hDKbNbPyDWQ3/ofAJQe9dwuwKiJOA1YVr82sg9UMe0SsAQ6+HvQKYHExvRi4sty2zKxs9V4bPzIiegAiokfSSdVmlDQbmF3nesysJE2/ESYiFgGLwD/QmbVTvafetkkaBVA8by+vJTNrhnrDvhyYUUzPAJaV046ZNUvN3XhJS4ELgBMlbQW+B9wB/FjSLOBXwLXNbHKw27lzZ0PLf/DBB3Uve/311yfrjz76aLJea4x16xw1wx4R06uULiq5FzNrIl8ua5YJh90sEw67WSYcdrNMOOxmmfAtroPA0KFDq9aefPLJ5LLnn39+sn7ppZcm60899VSybq3nIZvNMuewm2XCYTfLhMNulgmH3SwTDrtZJhx2s0z4PPsgN378+GR93bp1yXpvb2+y/swzzyTr3d3dVWv33XdfctlW/r85mPg8u1nmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCZ9nz1xXV1ey/vDDDyfrw4YNq3vd8+bNS9aXLFmSrPf09NS97sHM59nNMuewm2XCYTfLhMNulgmH3SwTDrtZJhx2s0z4PLslnXXWWcn6Pffck6xfdFH9g/0uXLgwWZ8/f36y/s4779S97sNZ3efZJT0kabukDX3eu1XSO5LWF49pZTZrZuUbyG78D4FL+nn/XyNiYvH4SbltmVnZaoY9ItYAO1rQi5k1USM/0M2R9Gqxm39CtZkkzZbULan6HyMzs6arN+z3A+OBiUAPcHe1GSNiUURMiohJda7LzEpQV9gjYltE7IuI/cADwORy2zKzstUVdkmj+rzsAjZUm9fMOkPN8+ySlgIXACcC24DvFa8nAgFsAW6IiJo3F/s8++AzfPjwZP3yyy+vWqt1r7zU7+niz6xevTpZnzp1arI+WFU7z37kABac3s/bDzbckZm1lC+XNcuEw26WCYfdLBMOu1kmHHazTPgWV2ubTz75JFk/8sj0yaK9e/cm6xdffHHV2rPPPptc9nDmPyVtljmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2Wi5l1vlrezzz47Wb/mmmuS9XPOOadqrdZ59Fo2btyYrK9Zs6ahzx9svGU3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLh8+yD3IQJE5L1OXPmJOtXXXVVsn7yyScfck8DtW/fvmS9pyf918v3799fZjuHPW/ZzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNM1DzPLmkMsAQ4GdgPLIqI70saATwKjKUybPO3I+L95rWar1rnsqdP72+g3Ypa59HHjh1bT0ul6O7uTtbnz5+frC9fvrzMdga9gWzZ9wJ/HRF/CPwxcKOkM4BbgFURcRqwqnhtZh2qZtgjoici1hXTu4BNwGjgCmBxMdti4Mom9WhmJTikY3ZJY4GvAmuBkRHRA5V/EICTSu/OzEoz4GvjJR0LPAbcHBE7pX6Hk+pvudnA7PraM7OyDGjLLukoKkH/UUQ8Xry9TdKooj4K2N7fshGxKCImRcSkMho2s/rUDLsqm/AHgU0RcU+f0nJgRjE9A1hWfntmVpaaQzZLmgI8D7xG5dQbwDwqx+0/Bk4BfgVcGxE7anxWlkM2jxw5Mlk/44wzkvV77703WT/99NMPuaeyrF27Nlm/8847q9aWLUtvH3yLan2qDdlc85g9Iv4LqHaAflEjTZlZ6/gKOrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJ/ynpARoxYkTV2sKFC5PLTpw4MVkfN25cPS2V4oUXXkjW77777mR95cqVyfpHH310yD1Zc3jLbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlIpvz7Oeee26yPnfu3GR98uTJVWujR4+uq6eyfPjhh1VrCxYsSC57++23J+t79uypqyfrPN6ym2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZyOY8e1dXV0P1RmzcuDFZX7FiRbK+d+/eZD11z3lvb29yWcuHt+xmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSYGMj77GGAJcDKV8dkXRcT3Jd0KXA/8uph1XkT8pMZnZTk+u1krVRuffSBhHwWMioh1koYBLwFXAt8GdkfEXQNtwmE3a75qYa95BV1E9AA9xfQuSZuA9v5pFjM7ZId0zC5pLPBVYG3x1hxJr0p6SNIJVZaZLalbUndjrZpZI2ruxn82o3Qs8BwwPyIelzQSeA8I4B+o7OrPrPEZ3o03a7K6j9kBJB0FrABWRsQ9/dTHAisi4qwan+OwmzVZtbDX3I2XJOBBYFPfoBc/3B3QBWxotEkza56B/Bo/BXgeeI3KqTeAecB0YCKV3fgtwA3Fj3mpz/KW3azJGtqNL4vDbtZ8de/Gm9ng4LCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmWj1k83vAL/u8PrF4rxN1am+d2he4t3qV2dup1QotvZ/9CyuXuiNiUtsaSOjU3jq1L3Bv9WpVb96NN8uEw26WiXaHfVGb15/Sqb11al/g3urVkt7aesxuZq3T7i27mbWIw26WibaEXdIlkjZLelPSLe3ooRpJWyS9Jml9u8enK8bQ2y5pQ5/3Rkh6WtIbxXO/Y+y1qbdbJb1TfHfrJU1rU29jJD0jaZOk1yXdVLzf1u8u0VdLvreWH7NLGgL8ApgKbAVeBKZHxMaWNlKFpC3ApIho+wUYkr4B7AaWHBhaS9K/ADsi4o7iH8oTIuJvO6S3WznEYbyb1Fu1Ycb/nDZ+d2UOf16PdmzZJwNvRsRbEfEp8AhwRRv66HgRsQbYcdDbVwCLi+nFVP5nabkqvXWEiOiJiHXF9C7gwDDjbf3uEn21RDvCPhp4u8/rrXTWeO8BPCXpJUmz291MP0YeGGareD6pzf0crOYw3q100DDjHfPd1TP8eaPaEfb+hqbppPN/X4+IrwGXAjcWu6s2MPcD46mMAdgD3N3OZophxh8Dbo6Ine3spa9++mrJ99aOsG8FxvR5/SXg3Tb00a+IeLd43g48QeWwo5NsOzCCbvG8vc39fCYitkXEvojYDzxAG7+7Ypjxx4AfRcTjxdtt/+7666tV31s7wv4icJqkL0s6GvgOsLwNfXyBpKHFDydIGgp8i84bino5MKOYngEsa2Mvn9Mpw3hXG2acNn93bR/+PCJa/gCmUflF/n+Av29HD1X6Gge8Ujxeb3dvwFIqu3X/R2WPaBbwu8Aq4I3ieUQH9fZvVIb2fpVKsEa1qbcpVA4NXwXWF49p7f7uEn215Hvz5bJmmfAVdGaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJv4fwyqthAx6ULgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 0\n",
    "image = X_train[index].reshape(28,28)\n",
    "# X_train[index]: (784,)\n",
    "# image: (28, 28)\n",
    "plt.imshow(image, 'gray')\n",
    "plt.title('label : {}'.format(y_train[index]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe18793",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e740256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0\n",
    "\n",
    "X_train = X_train[:, np.newaxis, :, :]\n",
    "X_test = X_test[:, np.newaxis, :, :]\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64\n",
    "\n",
    "print(y_test_one_hot.shape) # (60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdda2e8",
   "metadata": {},
   "source": [
    "# Divide into training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3214287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 1, 28, 28) (48000, 10)\n",
      "(12000, 1, 28, 28) (12000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "\n",
    "print(X_train.shape, y_train.shape) # (48000, 784)\n",
    "print(X_val.shape, y_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57990b8b",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "## Calculation of output size after one-dimensional convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0136409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_out_shape(N_in, P, F, S):\n",
    "  N_out = ((N_in + 2 * P - F) / S) + 1\n",
    "  return int(N_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1952ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out size: 2\n"
     ]
    }
   ],
   "source": [
    "N_in = 3 #(height, width)\n",
    "P = 0\n",
    "F = 2\n",
    "S = 1\n",
    "\n",
    "print(\"out size:\", calc_out_shape(N_in, P, F, S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd7c0cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 1 Creation of a two-dimensional convolution layer\n",
    "## Let's create a class of 2D convolution layers. Just extend from the 1dCNN implementation to 2D.\n",
    "\n",
    "\n",
    "class Conv2d:\n",
    "    def __init__(self, activation, optimizer, filter_num, input_channel, filter_size, stride=1, pad=0):\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        self.filter_num = filter_num\n",
    "        self.input_channel = input_channel\n",
    "        self.filter_size = filter_size\n",
    "        if self.activation == 'sigmoid':\n",
    "            initializer = XavierInitializer(self.filter_num, self.input_channel, self.filter_size)\n",
    "            self.W = initializer.W(_, _)\n",
    "            self.b = initializer.B(_)            \n",
    "        elif self.activation == 'tanh':\n",
    "            initializer = XavierInitializer(self.filter_num, self.input_channel, self.filter_size)\n",
    "            self.W = initializer.W(_, _)\n",
    "            self.b = initializer.B(_)          \n",
    "        elif self.activation == 'relu':\n",
    "            initializer = HeInitializer(self.filter_num, self.input_channel, self.filter_size)\n",
    "            self.W = initializer.W(_, _)\n",
    "            self.b = initializer.B(_)        \n",
    "        \n",
    "        ## Intermediate data (used during backward)\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        ## Gradient of weight / bias parameters\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def _calc_out_shape(self, H, FH, W, FW):\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        return out_h, out_w\n",
    "\n",
    "    def _im2col(self, input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "         ----------\n",
    "         input_data: Input data consisting of a 4-dimensional array of (number of data, channels, height, width)\n",
    "         filter_h: Filter height\n",
    "         filter_w: Filter width\n",
    "         stride: stride\n",
    "         pad: padding\n",
    "         Returns\n",
    "         -------\n",
    "         col: 2D array\n",
    "        \"\"\"\n",
    "        N, C, H, W = input_data.shape\n",
    "        out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "        out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "        img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "        col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "        for y in range(filter_h):\n",
    "            y_max = y + stride*out_h\n",
    "            for x in range(filter_w):\n",
    "                x_max = x + stride*out_w\n",
    "                col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "        \n",
    "        return col\n",
    "\n",
    "    def _col2im(self, col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "         ----------\n",
    "         col:\n",
    "         input_shape: Shape of input data (example: (10, 1, 28, 28))\n",
    "         filter_h:\n",
    "         filter_w\n",
    "         stride\n",
    "         pad\n",
    "         Returns\n",
    "         -------\n",
    "        \"\"\"\n",
    "        N, C, H, W = input_shape\n",
    "        out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "        out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "        col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "        img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "        for y in range(filter_h):\n",
    "            y_max = y + stride*out_h\n",
    "            for x in range(filter_w):\n",
    "                x_max = x + stride*out_w\n",
    "                img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "        return img[:, :, pad:H + pad, pad:W + pad]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape ##Filters (FN: number of filters, C: number of channels, FH: filter height, FW: filter width)\n",
    "        N, C, H, W = x.shape ##Input data (N: number of batches, C: number of channels, H: height, W: width)\n",
    "        \n",
    "        ## Output size after 2D convolution (h: height, w: width)\n",
    "        out_h, out_w = self._calc_out_shape(H, FH, W, FW)\n",
    "\n",
    "        ## Expansion of 4D data to 2D by im2col\n",
    "        col = self._im2col(x, FH, FW, self.stride, self.pad)\n",
    "        \n",
    "        col_W = self.W.reshape(FN, -1).T ## Transpose of filter\n",
    "        out = np.dot(col, col_W) + self.b ## Convolution operation\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) ## Output size formatting\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "        \n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "        \n",
    "        dcol = np.dot(dout, self.col_W.T)    \n",
    "        dx = self._col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef7c236",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "## Creation of maximum pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f76a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    def __init__(self, pool_h=3, pool_w=3, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def _im2col(self, input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "        \"\"\"\n",
    "       Parameters\n",
    "         ----------\n",
    "         input_data: Input data consisting of a 4-dimensional array of (number of data, channels, height, width)\n",
    "         filter_h: Filter height\n",
    "         filter_w: Filter width\n",
    "         stride: stride\n",
    "         pad: padding\n",
    "         Returns\n",
    "         -------\n",
    "         col: 2D array\n",
    "        \"\"\"\n",
    "        N, C, H, W = input_data.shape\n",
    "        out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "        out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "        img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "        col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "        for y in range(filter_h):\n",
    "            y_max = y + stride*out_h\n",
    "            for x in range(filter_w):\n",
    "                x_max = x + stride*out_w\n",
    "                col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "        \n",
    "        return col\n",
    "        \n",
    "    def _col2im(self, col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "        \"\"\"\n",
    "       Parameters\n",
    "         ----------\n",
    "         col:\n",
    "         input_shape: Shape of input data (example: (10, 1, 28, 28))\n",
    "         filter_h:\n",
    "         filter_w\n",
    "         stride\n",
    "         pad\n",
    "         Returns\n",
    "         -------\n",
    "        \"\"\"\n",
    "        N, C, H, W = input_shape\n",
    "        out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "        out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "        col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "        img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "        for y in range(filter_h):\n",
    "            y_max = y + stride*out_h\n",
    "            for x in range(filter_w):\n",
    "                x_max = x + stride*out_w\n",
    "                img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "        return img[:, :, pad:H + pad, pad:W + pad]\n",
    "                \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = self._im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = self._col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206cfdb",
   "metadata": {},
   "source": [
    "# Problem 5\n",
    "## Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42489bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    def __init__(self):\n",
    "        self.X_shape = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        ## One-dimensional\n",
    "        X_1d = X.reshape(X.shape[0], -1)\n",
    "        \n",
    "        ## Record of shape\n",
    "        self.X_shape = X.shape\n",
    "        \n",
    "        return X_1d    \n",
    "\n",
    "    def backward(self, X):\n",
    "        ## Return of shape\n",
    "        X = X.reshape(self.X_shape)\n",
    "        \n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7785d059",
   "metadata": {},
   "source": [
    "# Problem 6\n",
    "## Learning and estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ddfa9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "   Iterator to get a mini-batch\n",
    "\n",
    "     Parameters\n",
    "     ----------\n",
    "     X: ndarray, shape (n_samples, n_features) of the following form\n",
    "       Training data\n",
    "     y: ndarray of the following form, shape (n_samples, 1)\n",
    "       Correct answer value\n",
    "     batch_size: int\n",
    "       Batch size\n",
    "     seed: int\n",
    "       NumPy random number seed\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94aa2579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scratch2dCNNClassifier:\n",
    "    \"\"\"\n",
    "    Deep neural network classifier\n",
    "\n",
    "     Parameters\n",
    "     --------------\n",
    "     activaiton: {'sigmoid','tanh','relu'}\n",
    "         Types of activation functions\n",
    "     n_nodes: list\n",
    "         Node configuration example [400, 200, 100]\n",
    "     n_output: int\n",
    "         Number of output layers\n",
    "     alpha: float\n",
    "         Learning rate\n",
    "     optimizer: {'sgd','adagrad'}\n",
    "         Types of optimization methods\n",
    "     filter_num: int\n",
    "         Number of filters\n",
    "     filter_size: int\n",
    "         Filter size\n",
    "     stride: int (initial value: 1)\n",
    "         stride\n",
    "     pad: int (initial value: 0)\n",
    "         Padding\n",
    "        \n",
    "    Attributes\n",
    "     -------------\n",
    "     FC [n_layers]: dict\n",
    "         A dictionary that manages the connection layer\n",
    "     activation: dict\n",
    "         A dictionary that manages the activation function\n",
    "     self.epochs: int\n",
    "         Number of epochs (initial value: 10)\n",
    "     self.batch_size: int\n",
    "         Batch size (initial value: 20)\n",
    "     self.n_features: int\n",
    "         Number of features\n",
    "     self.val_is_true: boolean\n",
    "         Presence or absence of verification data\n",
    "     self.loss: empty ndarray\n",
    "         Record losses on training data\n",
    "     self.loss_val: empty ndarray\n",
    "         Record loss on validation data       \n",
    "    \"\"\"    \n",
    "    def __init__(self, activation, n_nodes, n_output, lr, optimizer, filter_num, filter_size):\n",
    "        self.select_activation = activation\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_output = n_output\n",
    "        self.lr = lr\n",
    "        self.select_optimizer = optimizer\n",
    "        \n",
    "        self.filter_num  = filter_num    ## Number of filters\n",
    "        self.filter_size   = filter_size    ## Filter size        \n",
    "        self.stride          = 1               ##stride\n",
    "        self.pad             = 0               ##Padding\n",
    "            \n",
    "    def _initialize_n_layers(self):\n",
    "        \"\"\"\n",
    "        Initialize the N layer.\n",
    "         When the sigmoid function and tanh function are activation functions: Xavier is the initial value\n",
    "         If the ReLU function is an activation function: He is the initial value\n",
    "        \"\"\"\n",
    "        self.activation = dict()\n",
    "        self.FC = dict()\n",
    "        ##Instantiation of FC class that connects the pooling layer to the fully connected layer\n",
    "        if self.select_activation == 'sigmoid':\n",
    "            self.FC[0] = FC(self.out_size, self.n_nodes[0], \n",
    "                            XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "            self.activation[0] = Sigmoid()\n",
    "        elif self.select_activation == 'tanh':\n",
    "            self.FC[0] = FC(self.out_size, self.n_nodes[0], \n",
    "                            XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "            self.activation[0] = Tanh()\n",
    "        elif self.select_activation == 'relu':\n",
    "            self.FC[0] = FC(self.out_size, self.n_nodes[0], \n",
    "                            HeInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "            self.activation[0] = ReLU()\n",
    "\n",
    "        ##Instantiation of FC class between fully coupled layers\n",
    "        for n_layer in range(len(self.n_nodes)):            \n",
    "            if n_layer == len(self.n_nodes) -1:\n",
    "                if self.select_activation == 'sigmoid':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_output, \n",
    "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = Softmax()\n",
    "                elif self.select_activation == 'tanh':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_output,\n",
    "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = Softmax()\n",
    "                elif self.select_activation == 'relu':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_output,\n",
    "                                              HeInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = Softmax()\n",
    "            else:\n",
    "                if self.select_activation == 'sigmoid':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_nodes[n_layer+1], \n",
    "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = Sigmoid()\n",
    "                elif self.select_activation == 'tanh':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_nodes[n_layer+1],\n",
    "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = Tanh()\n",
    "                elif self.select_activation == 'relu':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_nodes[n_layer+1],\n",
    "                                              HeInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = ReLU()\n",
    "                    \n",
    "    def _calc_out_shape(self, H, FH, W, FW, layer):\n",
    "        \"\"\"\n",
    "        A function that calculates the output size of the Convolution and Pooling layers\n",
    "        \"\"\"        \n",
    "        if layer == 'conv':\n",
    "            out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "            out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "        elif layer == 'pool':\n",
    "            out_h = int(1 + (H - FH) / self.stride)\n",
    "            out_w = int(1 + (W - FW) / self.stride)\n",
    "        \n",
    "        return out_h, out_w    \n",
    "    \n",
    "    def fit(self, X, y, epochs=10, batch_size=20):  \n",
    "        self.epochs = epochs                            ## Number of epochs    \n",
    "        self.batch_size = batch_size               ## Batch size\n",
    "        self.loss = np.zeros(self.epochs)        ## For output of learning curve / objective function (training data)\n",
    "        self.loss_val = np.zeros(self.epochs) ## For output of learning curve / objective function (verification data)        \n",
    "        \n",
    "        ##Instantiation of optimization class\n",
    "        if self.select_optimizer == 'sgd':\n",
    "            self.optimizer = SGD(self.lr)\n",
    "        elif self.select_optimizer == 'adagrad':\n",
    "            self.optimizer = AdaGrad(self.lr)            \n",
    "        \n",
    "        ##Input data shape\n",
    "        self.input_channel = X.shape[1]\n",
    "        self.input_h = X.shape[2]\n",
    "        self.input_w = X.shape[3]\n",
    "        \n",
    "        ##Instantiation of convolution layer class\n",
    "        self.conv = Conv2d(self.select_activation, self.optimizer, self.filter_num, self.input_channel, self.filter_size)\n",
    "        \n",
    "        ##Instantiation of activation function class\n",
    "        if self.select_activation == 'sigmoid':\n",
    "            self.activation_conv = Sigmoid()\n",
    "        elif self.select_activation == 'tanh':\n",
    "            self.activation_conv = Tanh()\n",
    "        elif self.select_activation == 'relu':\n",
    "            self.activation_conv = ReLU()\n",
    "            \n",
    "        ##Instantiation of pooling layer class\n",
    "        self.pool = MaxPool2D()   \n",
    "\n",
    "        ## Instantiation of smoothing class\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        ##Size before full connection\n",
    "        out_h, out_w = self._calc_out_shape(self.input_h, self.filter_size, self.input_w, self.filter_size, 'conv')\n",
    "        out_h, out_w = self._calc_out_shape(out_h, self.filter_size, out_w, self.filter_size, 'pool')\n",
    "        self.out_size = out_h * out_w * self.filter_num\n",
    "        \n",
    "        ##Initialization of N layer\n",
    "        self._initialize_n_layers()\n",
    "        \n",
    "        ##Get a mini-batch\n",
    "        get_mini_batch = GetMiniBatch(X, y, self.batch_size)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            print(\"epoch \", epoch + 1 , \" processing . . .\")\n",
    "            for mini_X_train,  mini_y_train in get_mini_batch:\n",
    "                self.X_ = mini_X_train\n",
    "                self.y_ = mini_y_train\n",
    "                \n",
    "                ##Forward propagation\n",
    "                ##Convolution layer\n",
    "                self.A = self.conv.forward(self.X_)\n",
    "                ##Activation function\n",
    "                self.Z = self.activation_conv.forward(self.A)\n",
    "                ##Pooling layer\n",
    "                self.P = self.pool.forward(self.Z)\n",
    "                ##Smoothing\n",
    "                self.F = self.flatten.forward(self.P)\n",
    "                \n",
    "                self.A = self.FC[0].forward(self.F)\n",
    "                self.Z = self.activation[0].forward(self.A)       \n",
    "                for n_layer in range(1, len(self.n_nodes) + 1):\n",
    "                    self.A = self.FC[n_layer].forward(self.Z)\n",
    "                    self.Z = self.activation[n_layer].forward(self.A)\n",
    "                \n",
    "                ##Backpropagation\n",
    "                self.dA, self.loss[epoch] = self.activation[len(self.n_nodes)].backward(self.Z, self.y_) ##Final layer, cross entropy error                self.dZ = self.FC[len(self.n_nodes)].backward(self.dA) #最終層\n",
    "                for n_layer in reversed(range(0, len(self.n_nodes))): ##Final layer -1\n",
    "                    self.dA = self.activation[n_layer].backward(self.dZ)\n",
    "                    self.dZ = self.FC[n_layer].backward(self.dA)\n",
    "\n",
    "                ##Return of shape\n",
    "                self.dF = self.flatten.backward(self.dZ)\n",
    "                \n",
    "                ##Pooling layer\n",
    "                self.dP = self.pool.backward(self.dF)\n",
    "                ##Activation function\n",
    "                self.dA = self.activation_conv.backward(self.dP)\n",
    "                ##Convolution layer\n",
    "                self.dZ = self.conv.backward(self.dA)\n",
    "                \n",
    "    def predict(self,X):\n",
    "        ## Forward propagation\n",
    "        ##Convolution\n",
    "        self.A = self.conv.forward(X)\n",
    "        ##Activation function\n",
    "        self.Z = self.activation_conv.forward(self.A)\n",
    "        ##Pooling layer\n",
    "        self.P = self.pool.forward(self.Z)\n",
    "        ##Smoothing\n",
    "        self.F = self.flatten.forward(self.P)\n",
    "\n",
    "        self.A = self.FC[0].forward(self.F)\n",
    "        self.Z = self.activation[0].forward(self.A)    \n",
    "        for n_layer in range(1, len(self.n_nodes) + 1):\n",
    "            self.A = self.FC[n_layer].forward(self.Z)\n",
    "            self.Z = self.activation[n_layer].forward(self.A)\n",
    "        \n",
    "        return np.argmax(self.Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e9ace5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
    "     Parameters\n",
    "     ----------\n",
    "     n_nodes1: int\n",
    "       Number of nodes in the previous layer\n",
    "     n_nodes2: int\n",
    "       Number of nodes in the later layer\n",
    "     initializer: instance of initialization method\n",
    "     optimizer: instance of optimization method\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        ## Initialization\n",
    "         ## Initialize self.W and self.B using the initialr method\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward\n",
    "         Parameters\n",
    "         ----------\n",
    "         X: ndarray, shape (batch_size, n_nodes1) of the following form\n",
    "             input\n",
    "         Returns\n",
    "         ----------\n",
    "         A: ndarray, shape (batch_size, n_nodes2) of the following form\n",
    "             output        \"\"\"        \n",
    "        self.X = X\n",
    "        \n",
    "        return np.dot(self.X, self.W) + self.B\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "       Backward\n",
    "         Parameters\n",
    "         ----------\n",
    "         dA: ndarray, shape (batch_size, n_nodes2) of the following form\n",
    "             Gradient flowing from behind\n",
    "         Returns\n",
    "         ----------\n",
    "         dZ: ndarray, shape (batch_size, n_nodes1) of the following form\n",
    "             Gradient to flow forward\n",
    "        \"\"\"\n",
    "        self.dB = dA\n",
    "        self.dW = np.dot(self.X.T, dA)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "\n",
    "       ## Update\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c176dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavier class\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_num=None, input_channel=None, filter_size=None):        \n",
    "        self.filter_num = filter_num\n",
    "        self.input_channel = input_channel\n",
    "        self.filter_size = filter_size \n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "         Parameters\n",
    "         ----------\n",
    "         n_nodes1: int\n",
    "           Number of nodes in the previous layer\n",
    "         n_nodes2: int\n",
    "           Number of nodes in the later layer\n",
    "\n",
    "         Returns\n",
    "         ----------\n",
    "         W: ndarray, shape (n_nodes1, n_nodes2) of the following form\n",
    "             weight\n",
    "        \"\"\"     \n",
    "        if self.filter_num and self.input_channel and self.filter_size is not None: ## Convolution layer\n",
    "            W = np.random.randn(self.filter_num, self.input_channel, self.filter_size, self.filter_size)\n",
    "        else: ## Fully connected layer\n",
    "            W = np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)       \n",
    "        \n",
    "        return W\n",
    "        \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "         Parameters\n",
    "         ----------\n",
    "         n_nodes2: int\n",
    "           Number of nodes in the later layer\n",
    "\n",
    "         Returns\n",
    "         ----------\n",
    "         B: ndarray, shape (n_nodes2,) of the following form\n",
    "             bias\n",
    "        \"\"\"       \n",
    "        if self.filter_num and self.input_channel and self.filter_size is not None: ## Convolution layer\n",
    "            B = np.random.randn(self.filter_num)\n",
    "        else: ## Fully connected layer\n",
    "            B = np.zeros(n_nodes2)\n",
    "        \n",
    "        return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3bf64309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "   He class\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_num=None, input_channel=None, filter_size=None):        \n",
    "        self.filter_num = filter_num\n",
    "        self.input_channel = input_channel\n",
    "        self.filter_size = filter_size       \n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "         Parameters\n",
    "         ----------\n",
    "         n_nodes1: int\n",
    "           Number of nodes in the previous layer\n",
    "         n_nodes2: int\n",
    "           Number of nodes in the later layer\n",
    "\n",
    "         Returns\n",
    "         ----------\n",
    "         W: ndarray, shape (n_nodes1, n_nodes2) of the following form\n",
    "             weight\n",
    "        \"\"\"      \n",
    "        if self.filter_num and self.input_channel and self.filter_size is not None: ## Convolution layer\n",
    "            W = np.random.randn(self.filter_num, self.input_channel, self.filter_size, self.filter_size) * np.sqrt(2 / self.filter_num) \n",
    "        else: ## Fully connected layer\n",
    "            W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1)\n",
    "    \n",
    "        return W\n",
    "        \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "         Parameters\n",
    "         ----------\n",
    "         n_nodes2: int\n",
    "           Number of nodes in the later layer\n",
    "\n",
    "         Returns\n",
    "         ----------\n",
    "         B: ndarray, shape (n_nodes2,) of the following form\n",
    "             bias\n",
    "        \"\"\"      \n",
    "        if self.filter_num and self.input_channel and self.filter_size is not None: ## Convolution layer\n",
    "            B = np.random.randn(self.filter_num)\n",
    "        else: ## Fully connected layer\n",
    "            B = np.random.randn(n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ab85df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent\n",
    "     Parameters\n",
    "     ----------\n",
    "     lr: Learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases for a layer\n",
    "         Parameters\n",
    "         ----------\n",
    "         layer: Instance of the layer before update\n",
    "        \n",
    "         Returns\n",
    "         ----------\n",
    "         layer: Updated layer instance\n",
    "        \"\"\"\n",
    "        layer.W = layer.W - self.lr * layer.dW\n",
    "        layer.B = layer.B - self.lr * layer.dB.mean(axis=0)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80805e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGrad class\n",
    "     Parameters\n",
    "     ----------\n",
    "     alpha: Learning rate\n",
    "    \n",
    "     Attributes\n",
    "     -------------\n",
    "     lr: Learning rate\n",
    "     HW: int (initial value), ndarray\n",
    "     HB: int (initial value), ndarray\n",
    "    \"\"\"    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.HW= 0 ## Initial value: 0\n",
    "        self.HB = 0 ## Initial value: 0      \n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases for a layer\n",
    "         Parameters\n",
    "         ----------\n",
    "         layer: instance\n",
    "             Instance of the layer before update\n",
    "\n",
    "         Returns\n",
    "         ----------\n",
    "         layer: instance\n",
    "             Updated tier instance\n",
    "        \"\"\"\n",
    "        ## Initialization\n",
    "        self.HW = np.zeros_like(layer.W)\n",
    "        self.HB = np.zeros_like(layer.B)\n",
    "        \n",
    "        ## update\n",
    "        self.HW = self.HW + (layer.dW**2) #/ layer.dB.shape[0]\n",
    "        self.HB = self.HB + (layer.dB**2).mean(axis=0)\n",
    "        layer.W = layer.W - self.lr * 1 / np.sqrt(self.HW + 1e-7) * layer.dW #/ layer.dB.shape[0]\n",
    "        layer.B = layer.B - self.lr * 1 / np.sqrt(self.HB + 1e-7) * layer.dB.mean(axis=0)\n",
    "        \n",
    "        return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d7d3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        \n",
    "        self.Z = 1.0 / (1.0 + np.exp(-self.A))\n",
    "        \n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "\n",
    "        return dZ * (1 - self.Z) * self.Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea92b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        \n",
    "        return np.tanh(self.A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \n",
    "        return dZ * (1.0 - (np.tanh(self.A) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05554472",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):    \n",
    "        self.A = A\n",
    "      \n",
    "        return np.maximum(self.A, 0)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "            \n",
    "        return np.where(self.A > 0, dZ, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fcb34243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \n",
    "        return np.exp(A) / np.sum(np.exp(A), axis=1, keepdims=True)\n",
    "    \n",
    "    def backward(self, Z, y):\n",
    "        \n",
    "        dA = Z - y\n",
    "        \n",
    "        ## Cross entropy error\n",
    "        batch_size = y.shape[0]\n",
    "        loss = -np.sum(y * np.log(Z)) / batch_size\n",
    "\n",
    "        return dA, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82a427aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1  processing . . .\n",
      "epoch  2  processing . . .\n",
      "epoch  3  processing . . .\n",
      "epoch  4  processing . . .\n",
      "epoch  5  processing . . .\n",
      "epoch  6  processing . . .\n",
      "epoch  7  processing . . .\n",
      "epoch  8  processing . . .\n",
      "epoch  9  processing . . .\n",
      "epoch  10  processing . . .\n"
     ]
    }
   ],
   "source": [
    "model = Scratch2dCNNClassifier(activation='relu', n_nodes=[400, 200, 100], n_output=10, lr=0.001, optimizer='sgd', filter_num=3, filter_size=3)\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=20)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3694a79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9746\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.97      0.97      0.97      1032\n",
      "           3       0.97      0.97      0.97      1010\n",
      "           4       0.99      0.97      0.98       982\n",
      "           5       0.96      0.97      0.97       892\n",
      "           6       0.98      0.98      0.98       958\n",
      "           7       0.97      0.98      0.97      1028\n",
      "           8       0.97      0.96      0.96       974\n",
      "           9       0.96      0.97      0.96      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy : {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7717bd",
   "metadata": {},
   "source": [
    "# Problem 8 (advanced task) \n",
    "## LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d7d64afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f7fcc610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet(input_shape, num_classes):\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(6, kernel_size=5, strides=(1, 1), input_shape=input_shape, activation=\"relu\"))\n",
    "  model.add(Conv2D(16, kernel_size=5, strides=(1, 1), activation=\"relu\"))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(120, activation=\"relu\") )\n",
    "  model.add(Dense(84, activation=\"relu\"))\n",
    "  model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "afcb5f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset():\n",
    "  def __init__(self):\n",
    "    self.image_shape = (28, 28, 1)\n",
    "    self.num_classes = 10\n",
    "\n",
    "  def get_batch(self):\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train, X_test = [self.preprocess(d) for d in [X_train, X_test]]\n",
    "    y_train, y_test = [self.preprocess(d, label_data=True) for d in [y_train, y_test]]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "  def preprocess(self, data, label_data=False):\n",
    "    if label_data:\n",
    "      data = keras.utils.to_categorical(data, self.num_classes)\n",
    "    else:\n",
    "      data = data.astype(\"float32\") / 255\n",
    "      shape = (data.shape[0],) + self.image_shape\n",
    "      data = data.reshape(shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3afce718",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "  def __init__(self, model, loss, optimizer, logdir=\"logdir\"):\n",
    "    self.target = model\n",
    "    self.target.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    self.verbose = 1\n",
    "    self.log_dir = os.path.join(os.path.dirname(__file__), logdir)\n",
    "    if not os.path.exists(self.log_dir):\n",
    "      os.mkdir(self.log_dir)\n",
    "\n",
    "  def train(self, X_train, y_train, batch_size, epochs, validation_split):\n",
    "    self.target.fit(X_train, y_train, \n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=validation_split,\n",
    "                    callbacks=[TensorBoard(log_dir=self.log_dir)],\n",
    "                    verbose=self.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "95098ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    __file__\n",
    "except NameError:\n",
    "    __file__ = os.path.join(os.getcwd(),\"dummy\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "093487fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "  1/375 [..............................] - ETA: 11s - loss: 2.3112 - accuracy: 0.0938WARNING:tensorflow:From C:\\Users\\Andrew\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "  2/375 [..............................] - ETA: 4:46 - loss: 2.2772 - accuracy: 0.1445WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2412s vs `on_train_batch_end` time: 1.2456s). Check your callbacks.\n",
      "375/375 [==============================] - 44s 118ms/step - loss: 0.2226 - accuracy: 0.9342 - val_loss: 0.0887 - val_accuracy: 0.9737\n",
      "Epoch 2/12\n",
      "375/375 [==============================] - 40s 108ms/step - loss: 0.0627 - accuracy: 0.9810 - val_loss: 0.0605 - val_accuracy: 0.9828\n",
      "Epoch 3/12\n",
      "375/375 [==============================] - 40s 108ms/step - loss: 0.0412 - accuracy: 0.9872 - val_loss: 0.0508 - val_accuracy: 0.9846\n",
      "Epoch 4/12\n",
      "375/375 [==============================] - 41s 108ms/step - loss: 0.0281 - accuracy: 0.9914 - val_loss: 0.0496 - val_accuracy: 0.9851\n",
      "Epoch 5/12\n",
      "375/375 [==============================] - 41s 108ms/step - loss: 0.0199 - accuracy: 0.9936 - val_loss: 0.0568 - val_accuracy: 0.9841\n",
      "Epoch 6/12\n",
      "375/375 [==============================] - 40s 107ms/step - loss: 0.0151 - accuracy: 0.9947 - val_loss: 0.0464 - val_accuracy: 0.9865\n",
      "Epoch 7/12\n",
      "375/375 [==============================] - 41s 108ms/step - loss: 0.0108 - accuracy: 0.9962 - val_loss: 0.0629 - val_accuracy: 0.9848\n",
      "Epoch 8/12\n",
      "375/375 [==============================] - 42s 112ms/step - loss: 0.0093 - accuracy: 0.9970 - val_loss: 0.0549 - val_accuracy: 0.9862\n",
      "Epoch 9/12\n",
      "375/375 [==============================] - 40s 108ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 0.0745 - val_accuracy: 0.9821\n",
      "Epoch 10/12\n",
      "375/375 [==============================] - 41s 108ms/step - loss: 0.0111 - accuracy: 0.9961 - val_loss: 0.0585 - val_accuracy: 0.9869\n",
      "Epoch 11/12\n",
      "375/375 [==============================] - 40s 108ms/step - loss: 0.0057 - accuracy: 0.9981 - val_loss: 0.0675 - val_accuracy: 0.9861\n",
      "Epoch 12/12\n",
      "375/375 [==============================] - 41s 110ms/step - loss: 0.0067 - accuracy: 0.9976 - val_loss: 0.0666 - val_accuracy: 0.9872\n",
      "Test loss: 0.05900625139474869\n",
      "Test accuracy: 0.9866999983787537\n"
     ]
    }
   ],
   "source": [
    "dataset = MNISTDataset()\n",
    "\n",
    "model = lenet(dataset.image_shape, dataset.num_classes)\n",
    "\n",
    "X_train, y_train, X_test, y_test = dataset.get_batch()\n",
    "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=\"adam\", logdir=\"logdir32\")\n",
    "trainer.train(X_train, y_train, batch_size=128, epochs=12, validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3741acc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11252), started 0:00:11 ago. (Use '!kill 11252' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1cee3766498cf045\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1cee3766498cf045\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logdir32\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0d458c",
   "metadata": {},
   "source": [
    "# Problem 9 (Advance task) \n",
    "## Survey of famous image recognition models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c906f4",
   "metadata": {},
   "source": [
    "When building a CNN model, it is necessary to calculate in advance the number of features at the stage of input to the all-connected layer. In addition, when dealing with large models, the calculation of the number of parameters becomes a necessity due to memory and computation speed. The framework can show you the number of parameters for each layer, but you need to understand the meaning to be able to adjust it properly.\n",
    "\n",
    "Calculate the output size and the number of parameters for the following three convolutional layers. For the number of parameters also consider the bias term.\n",
    "\n",
    "1. input size : 144×144, 3 channels filter size : 3×3, 6 channels stride : 1 padding : none\n",
    "\n",
    "2. input size : 60 x 60, 24 channels filter size : 3 x 3, 48 channels stride : 1 padding : none\n",
    "\n",
    "3. input size : 20x20, 10 channels filter size : 3x3, 20 channels stride : 2 padding : none\n",
    "\n",
    "The last example is a case where the convolution cannot be done just right. The last example is a case where the convolution can't be done just right, the framework may not see the extra pixels. This is an example of why such a setting is undesirable, as it will result in missing edges.\n",
    "\n",
    "1. input size : 144×144, 3 channels filter size : 3×3, 6 channels stride : 1 padding : none\n",
    "Answer Output size = 141 x 141 Number of parameters = 6 (number of output filters) x 3 (number of kernels) x 3 x 3 Bias term = 6\n",
    "\n",
    "1. input size : 60×60, 24 channels filter size : 3×3, 48 channels stride : 1 padding : none\n",
    "Answer Output size = 58 x 58 Number of parameters = 48 (number of output filters) x 24 (number of kernels) x 3 x 3 Bias term = 48\n",
    "\n",
    "1. input size : 20×20, 10 channels filter size : 3×3, 20 channels stride : 2 padding : none\n",
    "Answer Output size = 10 x 10 Number of parameters = 20 (number of output filters) x 10 (number of kernels) x 3 x 3 Bias term = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf733f",
   "metadata": {},
   "source": [
    "# Problem 10 \n",
    "## Calculation of output size and number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9cb52a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_out_shape_and_params(H, FH, W, FW, pad, stride, IC, FN):\n",
    "    out_h = int(1 + (H + 2*pad - FH) / stride)\n",
    "    out_w = int(1 + (W + 2*pad - FW) / stride)\n",
    "    \n",
    "    params = FH * FW * IC * FN + FN ## Finally add the bias term + FN\n",
    "        \n",
    "    activation_size = FN * out_h * out_w\n",
    "        \n",
    "    return out_h, out_w, params, activation_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e68154",
   "metadata": {},
   "source": [
    "Input size: 144 x 144, 3 channels\n",
    "\n",
    "\n",
    "Filter size: 3 x 3, 6 channels\n",
    "\n",
    "\n",
    "Stride: 1\n",
    "\n",
    "\n",
    "Padding: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "246bc0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size(FN × OH × OW) : 6 × 142 × 142\n",
      "Activation Size : 120984\n",
      "Number of parameters : 168\n"
     ]
    }
   ],
   "source": [
    "##Input size\n",
    "H, W, IC = (144, 144, 3) # IC = input_channel\n",
    "\n",
    "##Filter size\n",
    "FH, FW, FC = (3, 3, 3) ## FC (unused)= filter_channel\n",
    "\n",
    "##Number of filters (The filter size of 6 channels specified in the problem is assumed to be the number of filters)\n",
    "FN = 6\n",
    "\n",
    "##Stride, padding\n",
    "stride, pad = 1, 0\n",
    "\n",
    "out_h, out_w, params, activation_size = calc_out_shape_and_params(H, FH, W, FW, pad, stride, IC, FN)\n",
    "\n",
    "print('Output size(FN × OH × OW) : {} × {} × {}'.format(FN, out_h, out_w))\n",
    "print('Activation Size : {}'.format(activation_size))\n",
    "print('Number of parameters : {}'.format(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c135f70e",
   "metadata": {},
   "source": [
    "Input size: 60 x 60, 24 channels\n",
    "\n",
    "\n",
    "Filter size: 3 x 3, 48 channels\n",
    "\n",
    "\n",
    "Stride: 1\n",
    "\n",
    "\n",
    "Padding: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3101b781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size(FN × OH × OW) : 48 × 58 × 58\n",
      "Activation Size : 161472\n",
      "Number of parameters : 10416\n"
     ]
    }
   ],
   "source": [
    "##Input size\n",
    "H, W, IC = (60, 60, 24) ## IC = input_channel\n",
    "\n",
    "##Filter size\n",
    "FH, FW, FC = (3, 3, 24) ## FC(unused) = filter_channel\n",
    "\n",
    "##Number of filters (The problem-specified filter size of 48 channels is assumed to be the number of filters)\n",
    "FN = 48\n",
    "\n",
    "##Stride, padding\n",
    "stride, pad = 1, 0\n",
    "\n",
    "out_h, out_w, params, activation_size = calc_out_shape_and_params(H, FH, W, FW, pad, stride, IC, FN)\n",
    "\n",
    "print('Output size(FN × OH × OW) : {} × {} × {}'.format(FN, out_h, out_w))\n",
    "print('Activation Size : {}'.format(activation_size))\n",
    "print('Number of parameters : {}'.format(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ac9a4",
   "metadata": {},
   "source": [
    "Input size: 20 x 20, 10 channels\n",
    "\n",
    "\n",
    "Filter size: 3 x 3, 20 channels\n",
    "\n",
    "\n",
    "Stride: 2\n",
    "\n",
    "\n",
    "Padding: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f3969ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size(FN × OH × OW) : 20 × 9 × 9\n",
      "Activation Size : 1620\n",
      "Number of parameters : 1820\n"
     ]
    }
   ],
   "source": [
    "##Input size\n",
    "H, W, IC = (20, 20, 10) ## IC = input_channel\n",
    "\n",
    "##Filter size\n",
    "FH, FW, FC = (3, 3, 10) ## FC(unused) = filter_channel\n",
    "\n",
    "##Number of filters (The 20-channel filter size specified in the problem is assumed to be the number of filters)\n",
    "FN = 20\n",
    "\n",
    "##Stride, padding\n",
    "stride, pad = 2, 0\n",
    "\n",
    "out_h, out_w, params, activation_size = calc_out_shape_and_params(H, FH, W, FW, pad, stride, IC, FN)\n",
    "\n",
    "print('Output size(FN × OH × OW) : {} × {} × {}'.format(FN, out_h, out_w))\n",
    "print('Activation Size : {}'.format(activation_size))\n",
    "print('Number of parameters : {}'.format(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d01b8ed",
   "metadata": {},
   "source": [
    "Output size (FN x OH x OW): 20 x 9 x 9\n",
    "\n",
    "    \n",
    "Activation Size: 1620\n",
    "\n",
    "    \n",
    "Number of parameters: 1820"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e191c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
