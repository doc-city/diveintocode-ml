{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b8a0451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Andrew\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95869f",
   "metadata": {},
   "source": [
    "# Problem 1 \n",
    "## Looking back on scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748a096",
   "metadata": {},
   "source": [
    "(Example)\n",
    "\n",
    "1. I had to initialize the weights\n",
    "\n",
    "2. I needed an epoch loop\n",
    "\n",
    "Let's now remember how they are implemented in the framework.\n",
    "\n",
    "1. Need to implement Initializer class\n",
    "\n",
    "2. Optimizer implementation required\n",
    "\n",
    "3. It is necessary to implement the activation function\n",
    "\n",
    "4. Requires mini-batch size split\n",
    "\n",
    "5. Loss function needs to be defined and implemented\n",
    "\n",
    "6. Need to implement forward / back processing for class of layers such as convolution layer, fully connected layer, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58c9485",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "## Consider the correspondence between scratch and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5aa23b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 0.8680, val_loss : 1.0526, acc : 0.750, val_acc : 0.750\n",
      "Epoch 1, loss : 20.5905, val_loss : 9.4453, acc : 0.250, val_acc : 0.625\n",
      "Epoch 2, loss : 5.7928, val_loss : 11.4675, acc : 0.750, val_acc : 0.375\n",
      "Epoch 3, loss : 5.8330, val_loss : 3.6214, acc : 0.500, val_acc : 0.688\n",
      "Epoch 4, loss : 0.9386, val_loss : 2.9975, acc : 0.750, val_acc : 0.688\n",
      "Epoch 5, loss : 0.0101, val_loss : 1.2834, acc : 1.000, val_acc : 0.812\n",
      "Epoch 6, loss : 0.0000, val_loss : 0.2076, acc : 1.000, val_acc : 0.938\n",
      "Epoch 7, loss : 0.0000, val_loss : 0.0000, acc : 1.000, val_acc : 1.000\n",
      "Epoch 8, loss : 0.0000, val_loss : 0.0043, acc : 1.000, val_acc : 1.000\n",
      "Epoch 9, loss : 0.0000, val_loss : 0.0198, acc : 1.000, val_acc : 1.000\n",
      "test_acc : 0.850\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Binary classification of Iris dataset using neural network implemented in TensorFlow\n",
    "\"\"\"\n",
    "## Data set loading\n",
    "dataset_path =\"datasets_19_420_Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "## Condition extraction from data frame\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "## Convert labels to numbers\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "## Divided into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "## Further divided into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    Iterator to get a mini-batch\n",
    "\n",
    "     Parameters\n",
    "     ----------\n",
    "     X: ndarray, shape (n_samples, n_features) of the following form\n",
    "       Training data\n",
    "     y: ndarray, shape (n_samples, 1) of the following form\n",
    "       Correct answer value\n",
    "     batch_size: int\n",
    "       Batch size\n",
    "     seed: int\n",
    "       NumPy random number seed\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "## Hyperparameter settings\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "## Determine the shape of the arguments to pass to the calculation graph\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "## train mini batch iterator\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    Simple 3-layer neural network\n",
    "    \"\"\"\n",
    "    ## Declaration of weight and bias\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] ## tf.add and + are equivalent\n",
    "    return layer_output\n",
    "## Read network structure                              \n",
    "logits = example_net(X)\n",
    "## Objective function\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "## Optimization method\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "## Estimated result\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "## Index value calculation\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "## Initialization of variable\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "## Execution of calculation graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        ## Loop for each epoch\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            ## Loop for each mini-batch\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce3af8",
   "metadata": {},
   "source": [
    "# Problem 3 \n",
    "## Create a model of Iris using all three types of objective variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc21c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Andrew\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch 0, loss : 0.5979, val_loss : 0.7092, acc : 0.833, val_acc : 0.792\n",
      "Epoch 1, loss : 0.0599, val_loss : 0.5213, acc : 1.000, val_acc : 0.750\n",
      "Epoch 2, loss : 0.0624, val_loss : 0.7643, acc : 1.000, val_acc : 0.792\n",
      "Epoch 3, loss : 0.0083, val_loss : 0.3113, acc : 1.000, val_acc : 0.833\n",
      "Epoch 4, loss : 0.0146, val_loss : 0.3688, acc : 1.000, val_acc : 0.917\n",
      "Epoch 5, loss : 0.0053, val_loss : 0.2729, acc : 1.000, val_acc : 0.917\n",
      "Epoch 6, loss : 0.0043, val_loss : 0.2878, acc : 1.000, val_acc : 0.917\n",
      "Epoch 7, loss : 0.0033, val_loss : 0.2723, acc : 1.000, val_acc : 0.917\n",
      "Epoch 8, loss : 0.0027, val_loss : 0.2891, acc : 1.000, val_acc : 0.917\n",
      "Epoch 9, loss : 0.0019, val_loss : 0.3010, acc : 1.000, val_acc : 0.917\n",
      "test_acc : 1.000\n"
     ]
    }
   ],
   "source": [
    "## Pretreatment process\n",
    "## Load dataset\n",
    "dataset_path =\"datasets_19_420_Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "## Condition extraction from data frame\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "## Convert labels to numbers\n",
    "y[y=='Iris-setosa'] = 0\n",
    "y[y=='Iris-versicolor'] = 1\n",
    "y[y=='Iris-virginica'] = 2\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "## Divided into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "## Normalization and one-hot\n",
    "ss = StandardScaler().fit(X_train)\n",
    "X_train = ss.transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "ec = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "y_train = ec.fit_transform(y_train)\n",
    "y_test = ec.fit_transform(y_test)\n",
    "\n",
    "## Further divided into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "## Setting various parameters\n",
    "## Hyperparameter settings\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3\n",
    "## Determine the shape of the arguments to pass to the calculation graph\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "## train mini batch iterator\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "\n",
    "## Network construction \n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    Simple 3-layer neural network\n",
    "    \"\"\"\n",
    "    ## Initialization class definition\n",
    "    initializer = tf.keras.initializers.he_normal()\n",
    "    ## Declaration of weight and bias\n",
    "    weights = {\n",
    "        'w1': tf.Variable(initializer([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(initializer([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(initializer([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(initializer([n_hidden1])),\n",
    "        'b2': tf.Variable(initializer([n_hidden2])),\n",
    "        'b3': tf.Variable(initializer([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] ## tf.add and + are equivalent\n",
    "    return layer_output\n",
    "\n",
    "## Read network structure                               \n",
    "logits = example_net(X)\n",
    "## Objective function\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "## Optimization method\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "## Estimated result\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
    "## Index value calculation\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "## Initialization of variable\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "## Execution of calculation graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        ## Loop for each epoch\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            ## Loop for each mini-batch\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da0f970",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "## Create a model of House Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7222f081",
   "metadata": {},
   "source": [
    "# Data reading / preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a777240",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data set loading\n",
    "dataset_path =\"train.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "## Condition extraction from data frame\n",
    "y = df[\"SalePrice\"]\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.astype(np.float)[:, np.newaxis]\n",
    "\n",
    "## Divided into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "## Further divided into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "## Normalization\n",
    "x_scaler = StandardScaler()\n",
    "x_scaler.fit(X_train)\n",
    "X_train = x_scaler.transform(X_train)\n",
    "X_val   = x_scaler.transform(X_val)\n",
    "X_test  = x_scaler.transform(X_test)\n",
    "\n",
    "## Logarithmic transformation\n",
    "y_scaler = FunctionTransformer(func=np.log1p, inverse_func=np.expm1)\n",
    "y_train = y_scaler.transform(y_train)\n",
    "y_val   = y_scaler.transform(y_val)\n",
    "y_test  = y_scaler.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a6bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter settings\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_output = 1\n",
    "## Determine the shape of the arguments to pass to the calculation graph\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "Y = tf.placeholder(tf.float32, [None,1])\n",
    "## train mini batch iterator\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a9bb13",
   "metadata": {},
   "source": [
    "# Network & Computation Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc95106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    Simple 3-layer neural network\n",
    "    \"\"\"\n",
    "    initializer = tf.keras.initializers.he_normal()\n",
    "\n",
    "    ## Declaration of Mito Bias\n",
    "    weights = {\n",
    "        'w1': tf.Variable(initializer([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(initializer([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(initializer([n_hidden2, n_output]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(initializer([n_hidden1])),\n",
    "        'b2': tf.Variable(initializer([n_hidden2])),\n",
    "        'b3': tf.Variable(initializer([n_output]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] ## tf.add and + are equivalent\n",
    "    return layer_output\n",
    "\n",
    "## Read network structure                               \n",
    "logits = example_net(X)\n",
    "\n",
    "## Objective Function (MSE)\n",
    "loss_op = tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
    "## Optimization method\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "## Index value calculation\n",
    "rmse = tf.log(tf.reduce_mean(tf.square(tf.math.expm1(Y)  - tf.math.expm1(logits))))\n",
    "## Initialization of variable\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7809ed65",
   "metadata": {},
   "source": [
    "# Execution of calculation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbe4d9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 0.1937, val_loss : 0.5591, acc : 22.307, val_acc : 27.735\n",
      "Epoch 1, loss : 0.0160, val_loss : 0.1134, acc : 19.786, val_acc : 22.489\n",
      "Epoch 2, loss : 0.0078, val_loss : 0.0767, acc : 19.221, val_acc : 21.867\n",
      "Epoch 3, loss : 0.0472, val_loss : 0.1285, acc : 21.444, val_acc : 22.781\n",
      "Epoch 4, loss : 0.0537, val_loss : 0.1312, acc : 21.514, val_acc : 22.623\n",
      "Epoch 5, loss : 0.0308, val_loss : 0.1032, acc : 20.831, val_acc : 22.072\n",
      "Epoch 6, loss : 0.0200, val_loss : 0.0798, acc : 20.376, val_acc : 21.543\n",
      "Epoch 7, loss : 0.0215, val_loss : 0.0776, acc : 20.453, val_acc : 21.507\n",
      "Epoch 8, loss : 0.0241, val_loss : 0.0785, acc : 20.568, val_acc : 21.581\n",
      "Epoch 9, loss : 0.0236, val_loss : 0.0750, acc : 20.570, val_acc : 21.583\n",
      "test_score : 22.523\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        ## Loop for each epoch\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_score = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            ## Loop for each mini-batch\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, score = sess.run([loss_op, rmse], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_score += score\n",
    "        total_loss /= n_samples\n",
    "        total_score /= n_samples\n",
    "        val_loss, val_score = sess.run([loss_op, rmse], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, score, val_score))\n",
    "    test_score = sess.run(rmse, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_score : {:.3f}\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570bf0ce",
   "metadata": {},
   "source": [
    "# Problem 5\n",
    "## Create a model of MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae66345",
   "metadata": {},
   "source": [
    "# Data reading / preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8f79983",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "375bffae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (54000, 784)\n",
      "y shape: (54000, 10)\n"
     ]
    }
   ],
   "source": [
    "## Size specification\n",
    "img_heigt = 28\n",
    "img_width = 28\n",
    "feature_size = int(img_heigt * img_width)\n",
    "\n",
    "## Dimensionality reduction\n",
    "X_train = X_train.reshape(-1, feature_size)\n",
    "X_test  = X_test.reshape(-1, feature_size)\n",
    "\n",
    "## Type conversion for normalization\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test  = X_test.astype(np.float32)\n",
    "\n",
    "## Normalization\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "## One hot conversion\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "print(\"X shape:\", X_train.shape)\n",
    "print(\"y shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be04413",
   "metadata": {},
   "source": [
    "Setting of various parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d612675",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01 ## Learning rate\n",
    "batch_size = 2700 ## Batch size\n",
    "num_epochs = 10  ## Number of epochs\n",
    "n_hidden1 = 256  ## Hidden layer 1\n",
    "n_hidden2 = 128  ## Hidden layer 2\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_output = y_train.shape[1]\n",
    "## Determine the shape of the arguments to pass to the calculation graph\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "Y = tf.placeholder(tf.float32, [None, n_output])\n",
    "## train mini batch iterator\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f20304dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(x):\n",
    "  initializer = tf.keras.initializers.he_normal()\n",
    "  ## Declaration of weight and bias\n",
    "  weights = {\n",
    "      \"w1\": tf.Variable(initializer([n_input, n_hidden1])),\n",
    "      \"w2\": tf.Variable(initializer([n_hidden1, n_hidden2])),\n",
    "      \"w3\": tf.Variable(initializer([n_hidden2, n_output]))\n",
    "  }\n",
    "\n",
    "  biases = {\n",
    "      \"b1\": tf.Variable(initializer([n_hidden1,])),\n",
    "      \"b2\": tf.Variable(initializer([n_hidden2,])),\n",
    "      \"b3\": tf.Variable(initializer([n_output,]))\n",
    "  }\n",
    "\n",
    "  ## Forward propagation\n",
    "  layer_1 = tf.matmul(x, weights[\"w1\"]) + biases[\"b1\"]\n",
    "  layer_1 = tf.nn.relu(layer_1)\n",
    "  layer_2 = tf.matmul(layer_1, weights[\"w2\"]) + biases[\"b2\"]\n",
    "  layer_2 = tf.nn.relu(layer_2)\n",
    "  layer_output = tf.matmul(layer_2, weights[\"w3\"]) + biases[\"b3\"]\n",
    "  return layer_output\n",
    "\n",
    "logits = network(X)\n",
    "\n",
    "## Loss function\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "\n",
    "## optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "## Estimated result\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
    "\n",
    "## Index value calculation\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "## Initialization of variable\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d3cf594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 0.3042, val_loss : 0.2695, acc : 0.906, val_acc : 0.916\n",
      "Epoch 1, loss : 0.1810, val_loss : 0.1605, acc : 0.945, val_acc : 0.951\n",
      "Epoch 2, loss : 0.1195, val_loss : 0.1181, acc : 0.961, val_acc : 0.964\n",
      "Epoch 3, loss : 0.0868, val_loss : 0.0981, acc : 0.970, val_acc : 0.970\n",
      "Epoch 4, loss : 0.0628, val_loss : 0.0874, acc : 0.983, val_acc : 0.975\n",
      "Epoch 5, loss : 0.0474, val_loss : 0.0854, acc : 0.985, val_acc : 0.975\n",
      "Epoch 6, loss : 0.0367, val_loss : 0.0807, acc : 0.990, val_acc : 0.976\n",
      "Epoch 7, loss : 0.0354, val_loss : 0.0849, acc : 0.989, val_acc : 0.975\n",
      "Epoch 8, loss : 0.0309, val_loss : 0.0948, acc : 0.991, val_acc : 0.973\n",
      "Epoch 9, loss : 0.0302, val_loss : 0.0920, acc : 0.990, val_acc : 0.973\n",
      "test_acc : 0.974\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        ## Loop for each epoch\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            ## Loop for each mini-batch\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d2408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
