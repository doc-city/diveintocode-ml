{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) What kind of method existed in the field of object detection?\n",
    "\n",
    "\n",
    "SPPNet (Reference: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition)\n",
    "\n",
    "A method to improve the recognition accuracy of images of any size / scale regardless of the size of the input image. By using the spatial pyramid pooling layer, it is also possible to recognize rectangular objects, so it is also used in object detection technology.\n",
    "R-CNN (Reference: Contextual Action Recognition with R * CNN)\n",
    "\n",
    "When recognizing the behavior of a person or the attribute of an object in an image, not only the localized information itself but also the surrounding information is included as a feature to perform image recognition that takes into account the context and background in the image. Method\n",
    "[Rationale in the treatise]\n",
    "\n",
    "\n",
    "Abstract:\n",
    "(2 ~ 3) Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) There is Faster, but how did you speed it up?\n",
    "\n",
    "Since the time spent on region proposal was the biggest bottleneck in the conventional method, the region proposal method by Selective Search was replaced with a small convolutional network called RPN (Region Proposal Network), and the region proposal was performed on the RPN. Achieved near real-time calculation speed by performing object detection\n",
    "[Rationale in the treatise]\n",
    "\n",
    "Introduction: Introduction:\n",
    "(page1) The latest incarnation, Fast R-CNN [2], achieves near real-time rates using very deep networks [3], when ignoring the time spent on region proposals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) What is the difference between the One-Stage method and the Two-Stage method?\n",
    "Object detection methods using neural networks are classified into One-Stage Detection and Two-Stage Detection, respectively, depending on whether the detection is performed in one step or two steps. One-Stage Detection performs Bounding Box generation and class prediction at the same time. One-Stage Detection has a simple structure and can be expected to speed up learning and inference. Detection accuracy tends to be inferior to Two-Stage Detection. In Two-Stage Detection, Bounding Box candidates are first extracted in the first stage. Then, in the second stage, class prediction and BoundingBox modification are performed, and the final prediction result is generated.\n",
    "[Rationale in the treatise]\n",
    "\n",
    "\n",
    "(page10) The OverFeat paper [9] proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps. OverFeat is a one-stage, class-specific detection pipeline, and ours is a two-stage cascade consisting of class -agnostic proposals and class-specific detections.\n",
    "[Reference treatise]\n",
    "\n",
    "\n",
    "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) What is RPN?\n",
    "\n",
    "RPN is a machine learning model that can detect \"where an object appears in a certain image\" = \"where the object appears and its rectangular shape\".\n",
    "[Rationale in the treatise]\n",
    "\n",
    "\n",
    "(4) What is RPN?\n",
    "[Solution]\n",
    "\n",
    "RPN is a machine learning model that can detect \"where an object appears in a certain image\" = \"where the object appears and its rectangular shape\".\n",
    "[Rationale in the treatise]\n",
    "\n",
    "Introduction\n",
    "(page1) The RPN is thus a kind of fully convolutional network (FCN) [7] and can be trained end-toend specifically for the task for generating detection proposals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) What is RoI pooling?\n",
    "\n",
    "RoI Pooling has the role of making the input to the classifying layer a fixed dimension. By cutting out the feature map corresponding to the area of the object candidate by RoI Pooling, the feature extraction is standardized and speeded up. A method of fixing dimensions even when the size of the image is different\n",
    "[Rationale in the treatise]\n",
    "\n",
    "\n",
    "(page6) The RoI pooling layer [2] in Fast R-CNN accepts the convolutional features and also the predicted bounding boxes as input, so a theoretically valid backpropagation solver should also involve gradients wrt the box coordinates. These gradients are ignored in the above In a non-approximate joint training solution, we need an RoI pooling layer that is differentiable wrt the box coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) What is the appropriate size of Anchor?\n",
    "\n",
    "Anchor is a rectangle that shows the area of an object, which is generated at regular intervals in the image and used in the Loss Function. The following 3 x 3 = 9 patterns are used in the paper experiments.\n",
    "\n",
    "\n",
    "Scale: $ 128 ^ 2 $, $ 256 ^ 2 $, $ 512 ^ 2 $\n",
    "Aspect ratio: 1: 1, 1: 2, 2: 1\n",
    "Anchor should be tuned depending on the domain.\n",
    "[Rationale in the treatise]\n",
    "\n",
    "\n",
    "(page6) For anchors, we use 3 scales with box areas of $ 128 ^ 2 $, $ 256 ^ 2 $, and $ 512 ^ 2 $ pixels, and 3 aspect ratios of 1: 1, 1: 2, and 2: 1. These hyper-parameters are not carefully chosen for a particular dataset, and we provide ablation experiments on their effects in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) What kind of data set is used and what kind of index value is obtained compared to the previous research?\n",
    "\n",
    "Trial dataset: MS COCO dataset\n",
    "\n",
    "\n",
    "Comparison results with previous studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 11: Object dection results (%) on the ms coco dataset. This model is VGG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"diver.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
