{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    Logistic regression scratch implementation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      Number of iterations\n",
    "    lr : float\n",
    "      Learning rate\n",
    "    no_bias : bool\n",
    "      If no bias term is included True\n",
    "    verbose : bool\n",
    "      When outputting the learning process True\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : The following form ndarray, shape (n_features,)\n",
    "      Parameters\n",
    "    self.loss : The following form ndarray, shape (self.iter,)\n",
    "      Record losses on training data\n",
    "    self.val_loss : The following form ndarray, shape (self.iter,)\n",
    "      Record loss on validation data\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iter=50, lr=0.01, no_bias=False, C=1.0, verbose=False, debug=False):\n",
    "        # Record hyperparameters as attributes\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.no_bias = no_bias\n",
    "        self.C = C\n",
    "        self.verbose = verbose\n",
    "        # Prepare an array to record the loss\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        \n",
    "        # Debug data \n",
    "        self.debug = debug\n",
    "        self.test_data = np.array([0.2, -0.3, 0.1]) \n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Learn logistic regression. If validation data is entered, the loss and accuracy for it are also calculated for each iteration.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following form ndarray, shape (n_samples, n_features)\n",
    "            Features of training data\n",
    "        y : The following form ndarray, shape (n_samples, )\n",
    "            Correct answer value of training data\n",
    "        X_val : The following form ndarray, shape (n_samples, n_features)\n",
    "            Features of verification data\n",
    "        y_val : The following form ndarray, shape (n_samples, )\n",
    "            Correct value of verification data\n",
    "        \"\"\"\n",
    "        \n",
    "        # Whether to calculate validation data\n",
    "        is_val_calc = (X_val is not None) and (y_val is not None)\n",
    "        \n",
    "        #Data preprocessing\n",
    "        X, y  = self._data_processing(X, y)\n",
    "        if is_val_calc:\n",
    "            X_val, y_val  = self._data_processing(X_val, y_val)\n",
    "        \n",
    "        #Parameter θ is created according to the number of features\n",
    "        if self.debug:\n",
    "            self.coef_ = self.test_data\n",
    "        else:\n",
    "            self.coef_ = np.array(np.random.normal(0, 1, X.shape[1]))\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"default theta:\", self.coef_)\n",
    "            print(\"default ploba:\", self.predict_proba(X[:, 1:]).flatten())\n",
    "            print()\n",
    "        \n",
    "        # Iterator number, repeat learning\n",
    "        for i in range(self.iter):\n",
    "            self._gradient_descent(X, y)\n",
    "            self.loss[i] = self._loss(X, y)\n",
    "            \n",
    "            if is_val_calc:\n",
    "                self.val_loss[i] = self._loss(X_val, y_val)\n",
    "            \n",
    "            if self.verbose:\n",
    "                #Output the learning process when verbose is set to True\n",
    "                print(\"iter: {}  coef:{}\".format(i+1, self.coef_))\n",
    "                print(\"probability:\", self.predict_proba(X[:, 1:]).flatten())\n",
    "                print(\"loss:\", self.loss)\n",
    "                print()\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Estimate the label using logistic regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following form ndarray, shape (n_samples, n_features)\n",
    "            sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            The following form ndarray, shape (n_samples, 1)\n",
    "            Estimated result by logistic regression\n",
    "        \"\"\"\n",
    "        \n",
    "        X, _ = self._data_processing(X)\n",
    "        pred = self._logistic_hypothesis(X)\n",
    "        pred = np.round(pred).astype(int).flatten()\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Estimate the probability using logistic regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following form ndarray, shape (n_samples, n_features)\n",
    "            sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            The following form ndarray, shape (n_samples, 1)\n",
    "            Estimated result by logistic regression\n",
    "        \"\"\"\n",
    "        X, _ = self._data_processing(X)\n",
    "        pred = self._logistic_hypothesis(X).flatten()\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def _data_processing(self, X, y=None):\n",
    "        # If the dataset is DataFrame, Series, convert to ndarray\n",
    "        if isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "            X = X.values\n",
    "        if isinstance(y, (pd.DataFrame, pd.Series)):\n",
    "            y = y.values\n",
    "        \n",
    "        # If the dataset is one-dimensional, convert it to two-dimensional\n",
    "        if X.shape[0] == X.size:\n",
    "            X = X.reshape(-1, 1)\n",
    "        \n",
    "        if y is not None:\n",
    "            y = y.reshape(-1, 1)\n",
    "\n",
    "        # Set the number of feature samples\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Addition of bias term\n",
    "        if self.no_bias:\n",
    "            n = X.shape[1]\n",
    "        else:\n",
    "            n = X.shape[1] + 1\n",
    "            # Add the intercept (x = 1) to X\n",
    "            X = np.concatenate((np.ones((m, 1)), X), axis=1)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def _sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "    def _logistic_hypothesis(self, X):\n",
    "        linear_format = np.sum(self.coef_* X, axis=1).reshape(-1, 1)\n",
    "        h_theta = self._sigmoid(linear_format)\n",
    "        \n",
    "        return h_theta\n",
    "    \n",
    "    def _gradient_descent(self, X, y):\n",
    "        \"\"\"\n",
    "        Update the parameter θ in the gradient direction\n",
    "        \n",
    "         Parameters\n",
    "        ----------\n",
    "        X : The following form ndarray, shape (n_samples, n_features)\n",
    "          Training data\n",
    "        y : The following form ndarray, shape (n_samples, )\n",
    "          Correct answer value of training data\n",
    "\n",
    "        \"\"\"\n",
    "        m = len(X)\n",
    "        \n",
    "        grad = np.average(self._error(X, y) * X, axis=0)\n",
    "        tmp = grad[0]\n",
    "        \n",
    "        grad = np.average(self._error(X, y) * X, axis=0) + (self.C * self.coef_ / m)\n",
    "        grad[0] = tmp\n",
    "        \n",
    "        self.coef_ = self.coef_ - self.lr * grad\n",
    "        \n",
    "    def _error(self, X, y):\n",
    "        \"\"\"\n",
    "        A function that calculates the error between the predicted value and the actual value\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following form ndarray, shape (n_samples, n_features)\n",
    "          Training data\n",
    "        y : The following form ndarray, shape (n_samples, )\n",
    "            Correct answer value of training data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "          The following form ndarray, shape (n_samples, 1)\n",
    "          Error between predicted value and actual value\n",
    "\n",
    "        \"\"\"\n",
    "        return self._logistic_hypothesis(X) - y\n",
    "\n",
    "    def _loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate the loss function\n",
    "\n",
    "         Parameters\n",
    "        ----------\n",
    "        X : The following form ndarray, shape (n_samples, n_features)\n",
    "          Training data\n",
    "        y : The following form ndarray, shape (n_samples, )\n",
    "          Correct answer value of training data\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        m = len(X)\n",
    "        start = 0 if self.no_bias else 1\n",
    "        \n",
    "        h_X = self._logistic_hypothesis(X)\n",
    "        loss = np.average((-y * np.log(h_X)) - ((1 - y) * np.log(1 - h_X)))\n",
    "        J_theta =  loss + self.C * np.sum(self.coef_[start:] ** 2) / (2 * m)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"loss\", loss)\n",
    "            print(\"J_theta:\", J_theta)\n",
    "        \n",
    "        return J_theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 \n",
    "\n",
    "## Hypothetical Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the method of the logistic regression assumption function in the ScratchLogisticRegression class.\n",
    "\n",
    "The logistic regression hypothetical function is the linear regression hypothetical function passed through his sigmoid function. The sigmoid function is expressed by the following formula.\n",
    "\n",
    "$$g(z) = \\frac{1}{1 + e Unknown Character Unknown Character Unknown Character ^ z}$$\n",
    "\n",
    "The hypothetical function for linear regression was:\n",
    "\n",
    "$$h_0(x) = 0^T . x$$ \n",
    "\n",
    "Putting it all together, the hypothetical function of logistic regression is as follows.\n",
    "\n",
    "$$h_0(x) = \\frac{1}{1 + e Unknown Character Unknown Character Unknown Character  0^T . x}$$\n",
    "\n",
    "$ x $: Feature vector\n",
    "\n",
    "\n",
    "$ 0 $: Parameter (weight) vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 \n",
    "\n",
    "\n",
    "## The Steepest Descent Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please implement to learn by the steepest descent method. Please add the method _gradient_descent of the parameter update expression expressed by the following expression and call it from the fit method.\n",
    "\n",
    "$a$: Learning rate\n",
    "\n",
    "$ i $: Sample index\n",
    "\n",
    "$ j $: Feature index\n",
    "\n",
    "$ m $: Number of data entered\n",
    "\n",
    "$h_0()$: Hypothetical function\n",
    "\n",
    "$ x $: Feature vector\n",
    "\n",
    "$0$: Parameter (weight) vector\n",
    "\n",
    "$x ^(i)$: Feature vector of i-th sample\n",
    "\n",
    "$y^(i)$: Correct label for i-th sample\n",
    "\n",
    "$0_j$: jth parameter (weight)\n",
    "\n",
    "Unknown Character Unknown Character: Regularization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 \n",
    "\n",
    "## Estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please implement the estimation mechanism. Add to the predict and predict_proba methods included in the template of the ScratchLogisticRegression class.\n",
    "\n",
    "Hypothetical function $ hθ (x) $ The output of is the return value of predict_proba, and the value is labeled as 1 and 0 with a threshold value, which is the return value of predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 \n",
    "\n",
    "## Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the objective function (loss function) of logistic regression expressed by the following formula.\n",
    "\n",
    "And be sure to record this in self.loss, self.val_loss.\n",
    "\n",
    "Note that this formula contains a regularization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default theta: [ 0.2 -0.3  0.1]\n",
      "default ploba: [0.83202 0.5     0.31003]\n",
      "\n",
      "loss 0.9812493147435388\n",
      "J_theta: 1.0106406921347924\n",
      "iter: 1  coef:[ 0.20119 -0.26423  0.13549]\n",
      "probability: [0.77636 0.55347 0.4255 ]\n",
      "loss: [1.01064 0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.8010915698514743\n",
      "J_theta: 0.8283149803176726\n",
      "iter: 2  coef:[ 0.20201 -0.23247  0.16621]\n",
      "probability: [0.71748 0.59915 0.53417]\n",
      "loss: [1.01064 0.82831 0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.6653787145913778\n",
      "J_theta: 0.691674472451689\n",
      "iter: 3  coef:[ 0.20251 -0.20458  0.19244]\n",
      "probability: [0.65932 0.63717 0.62587]\n",
      "loss: [1.01064 0.82831 0.69167 0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.5630919671449031\n",
      "J_theta: 0.5892820128296266\n",
      "iter: 4  coef:[ 0.20276 -0.18013  0.21476]\n",
      "probability: [0.60456 0.66853 0.69848]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.485047029219875\n",
      "J_theta: 0.5116604157792328\n",
      "iter: 5  coef:[ 0.20286 -0.1586   0.23385]\n",
      "probability: [0.55454 0.69446 0.75437]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.4244575535437783\n",
      "J_theta: 0.4518295031652749\n",
      "iter: 6  coef:[ 0.20285 -0.1395   0.25031]\n",
      "probability: [0.50965 0.71608 0.79711]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.3765468013921192\n",
      "J_theta: 0.40488650970651646\n",
      "iter: 7  coef:[ 0.20277 -0.12243  0.26463]\n",
      "probability: [0.46974 0.73428 0.82995]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.3379909164434191\n",
      "J_theta: 0.36742544275455574\n",
      "iter: 8  coef:[ 0.20266 -0.10704  0.27721]\n",
      "probability: [0.4344  0.74977 0.85546]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.30646218831510313\n",
      "J_theta: 0.33706450820306405\n",
      "iter: 9  coef:[ 0.20253 -0.09308  0.28834]\n",
      "probability: [0.40315 0.7631  0.87553]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.28030719217030803\n",
      "J_theta: 0.31211425594433184\n",
      "iter: 10  coef:[ 0.20239 -0.08033  0.29828]\n",
      "probability: [0.37549 0.77466 0.89154]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.25833130173544966\n",
      "J_theta: 0.291355834367839\n",
      "iter: 11  coef:[ 0.20225 -0.06861  0.30719]\n",
      "probability: [0.35096 0.78479 0.90448]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.2396561760378955\n",
      "J_theta: 0.27389451936968073\n",
      "iter: 12  coef:[ 0.20211 -0.05779  0.31524]\n",
      "probability: [0.32914 0.79372 0.91508]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.22362505892492757\n",
      "J_theta: 0.2590624692623725\n",
      "iter: 13  coef:[ 0.20199 -0.04775  0.32254]\n",
      "probability: [0.30967 0.80165 0.92385]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.20973907406199388\n",
      "J_theta: 0.24635335329075847\n",
      "iter: 14  coef:[ 0.20187 -0.03839  0.32919]\n",
      "probability: [0.29224 0.80874 0.93119]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.19761370887016774\n",
      "J_theta: 0.23537772429875328\n",
      "iter: 15  coef:[ 0.20176 -0.02964  0.33528]\n",
      "probability: [0.27658 0.81512 0.93739]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.1869486029606843\n",
      "J_theta: 0.22583205362175734\n",
      "iter: 16  coef:[ 0.20167 -0.02143  0.34087]\n",
      "probability: [0.26246 0.82087 0.94268]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.17750623335439095\n",
      "J_theta: 0.21747689756614663\n",
      "iter: 17  coef:[ 0.20158 -0.0137   0.34601]\n",
      "probability: [0.24968 0.8261  0.94723]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.16909664256401477\n",
      "J_theta: 0.21012126357658234\n",
      "iter: 18  coef:[ 0.2015  -0.0064   0.35076]\n",
      "probability: [0.23809 0.83086 0.95116]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.16156633559836872\n",
      "J_theta: 0.20361125175900663\n",
      "iter: 19  coef:[0.20144 0.00051 0.35515]\n",
      "probability: [0.22754 0.83522 0.9546 ]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.15479009681651068\n",
      "J_theta: 0.19782168954596643\n",
      "iter: 20  coef:[0.20138 0.00707 0.35923]\n",
      "probability: [0.2179  0.83922 0.95761]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.1486648814043389\n",
      "J_theta: 0.19264989212216133\n",
      "iter: 21  coef:[0.20133 0.0133  0.36301]\n",
      "probability: [0.20908 0.8429  0.96027]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.14310520104636396\n",
      "J_theta: 0.18801095314933294\n",
      "iter: 22  coef:[0.20129 0.01923 0.36653]\n",
      "probability: [0.20097 0.8463  0.96264]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.13803959954873468\n",
      "J_theta: 0.1838341512042165\n",
      "iter: 23  coef:[0.20126 0.0249  0.36982]\n",
      "probability: [0.19351 0.84945 0.96474]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.13340793308134383\n",
      "J_theta: 0.18006017939266422\n",
      "iter: 24  coef:[0.20123 0.03031 0.37288]\n",
      "probability: [0.18662 0.85238 0.96663]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.1291592510780877\n",
      "J_theta: 0.17663898910229728\n",
      "iter: 25  coef:[0.20121 0.03549 0.37574]\n",
      "probability: [0.18025 0.8551  0.96833]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.12525013025392986\n",
      "J_theta: 0.17352809673753158\n",
      "iter: 26  coef:[0.2012  0.04046 0.37841]\n",
      "probability: [0.17434 0.85763 0.96986]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.1216433538079326\n",
      "J_theta: 0.17069124290762236\n",
      "iter: 27  coef:[0.20119 0.04523 0.38092]\n",
      "probability: [0.16884 0.86    0.97125]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.11830685602127129\n",
      "J_theta: 0.16809732239248323\n",
      "iter: 28  coef:[0.20119 0.04981 0.38326]\n",
      "probability: [0.16373 0.86222 0.97251]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.11521287267457475\n",
      "J_theta: 0.1657195239334434\n",
      "iter: 29  coef:[0.2012  0.05422 0.38546]\n",
      "probability: [0.15896 0.8643  0.97367]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.11233725238528718\n",
      "J_theta: 0.16353463393582418\n",
      "iter: 30  coef:[0.20121 0.05847 0.38752]\n",
      "probability: [0.1545  0.86625 0.97472]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.10965889472741443\n",
      "J_theta: 0.16152246919451613\n",
      "iter: 31  coef:[0.20122 0.06257 0.38946]\n",
      "probability: [0.15033 0.86809 0.97569]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.10715928896177211\n",
      "J_theta: 0.15966541191083863\n",
      "iter: 32  coef:[0.20124 0.06652 0.39127]\n",
      "probability: [0.14642 0.86982 0.97658]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.10482213315377141\n",
      "J_theta: 0.15794802635831434\n",
      "iter: 33  coef:[0.20127 0.07034 0.39297]\n",
      "probability: [0.14274 0.87145 0.9774 ]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.10263301793607484\n",
      "J_theta: 0.15635674113906206\n",
      "iter: 34  coef:[0.20129 0.07404 0.39457]\n",
      "probability: [0.13929 0.87299 0.97816]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.15636 0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.10057916257470857\n",
      "J_theta: 0.15487958445092764\n",
      "iter: 35  coef:[0.20133 0.07761 0.39608]\n",
      "probability: [0.13603 0.87445 0.97887]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.15636 0.15488 0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.09864919359892758\n",
      "J_theta: 0.15350596244488962\n",
      "iter: 36  coef:[0.20136 0.08108 0.39749]\n",
      "probability: [0.13296 0.87582 0.97952]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.15636 0.15488 0.15351\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.09683295825958242\n",
      "J_theta: 0.1522264728001157\n",
      "iter: 37  coef:[0.2014  0.08443 0.39881]\n",
      "probability: [0.13006 0.87713 0.98013]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.15636 0.15488 0.15351\n",
      " 0.15223 0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.09512136663563182\n",
      "J_theta: 0.15103274723171378\n",
      "iter: 38  coef:[0.20144 0.08769 0.40006]\n",
      "probability: [0.12731 0.87837 0.9807 ]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.15636 0.15488 0.15351\n",
      " 0.15223 0.15103 0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.09350625742245298\n",
      "J_theta: 0.1499173178850976\n",
      "iter: 39  coef:[0.20149 0.09085 0.40122]\n",
      "probability: [0.12472 0.87954 0.98123]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.15636 0.15488 0.15351\n",
      " 0.15223 0.15103 0.14992 0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.09198028338935187\n",
      "J_theta: 0.14887350354357576\n",
      "iter: 40  coef:[0.20154 0.09391 0.40232]\n",
      "probability: [0.12226 0.88065 0.98172]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.15636 0.15488 0.15351\n",
      " 0.15223 0.15103 0.14992 0.14887 0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.09053681324738166\n",
      "J_theta: 0.14789531234395475\n",
      "iter: 41  coef:[0.20159 0.09689 0.40334]\n",
      "probability: [0.11992 0.88171 0.98218]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.15636 0.15488 0.15351\n",
      " 0.15223 0.15103 0.14992 0.14887 0.1479  0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.08916984726756629\n",
      "J_theta: 0.14697735830503011\n",
      "iter: 42  coef:[0.20164 0.09979 0.40431]\n",
      "probability: [0.11771 0.88272 0.98262]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.15636 0.15488 0.15351\n",
      " 0.15223 0.15103 0.14992 0.14887 0.1479  0.14698 0.      0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.08787394446821617\n",
      "J_theta: 0.1461147894609575\n",
      "iter: 43  coef:[0.2017  0.10261 0.40521]\n",
      "probability: [0.1156  0.88368 0.98303]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.15636 0.15488 0.15351\n",
      " 0.15223 0.15103 0.14992 0.14887 0.1479  0.14698 0.14611 0.      0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.08664415957438976\n",
      "J_theta: 0.14530322578242522\n",
      "iter: 44  coef:[0.20176 0.10535 0.40605]\n",
      "probability: [0.11359 0.88459 0.98341]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.15636 0.15488 0.15351\n",
      " 0.15223 0.15103 0.14992 0.14887 0.1479  0.14698 0.14611 0.1453  0.\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.08547598826276569\n",
      "J_theta: 0.14453870538382418\n",
      "iter: 45  coef:[0.20182 0.10802 0.40684]\n",
      "probability: [0.11168 0.88546 0.98377]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.15636 0.15488 0.15351\n",
      " 0.15223 0.15103 0.14992 0.14887 0.1479  0.14698 0.14611 0.1453  0.14454\n",
      " 0.      0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.08436531945673642\n",
      "J_theta: 0.14381763777006198\n",
      "iter: 46  coef:[0.20188 0.11063 0.40758]\n",
      "probability: [0.10986 0.88629 0.98411]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.15636 0.15488 0.15351\n",
      " 0.15223 0.15103 0.14992 0.14887 0.1479  0.14698 0.14611 0.1453  0.14454\n",
      " 0.14382 0.      0.      0.      0.     ]\n",
      "\n",
      "loss 0.08330839364143168\n",
      "J_theta: 0.1431367630845813\n",
      "iter: 47  coef:[0.20195 0.11316 0.40826]\n",
      "probability: [0.10812 0.88709 0.98443]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.15636 0.15488 0.15351\n",
      " 0.15223 0.15103 0.14992 0.14887 0.1479  0.14698 0.14611 0.1453  0.14454\n",
      " 0.14382 0.14314 0.      0.      0.     ]\n",
      "\n",
      "loss 0.08230176633600372\n",
      "J_theta: 0.14249311649009103\n",
      "iter: 48  coef:[0.20202 0.11563 0.4089 ]\n",
      "probability: [0.10646 0.88784 0.98474]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.15636 0.15488 0.15351\n",
      " 0.15223 0.15103 0.14992 0.14887 0.1479  0.14698 0.14611 0.1453  0.14454\n",
      " 0.14382 0.14314 0.14249 0.      0.     ]\n",
      "\n",
      "loss 0.08134227599820641\n",
      "J_theta: 0.1418839969530114\n",
      "iter: 49  coef:[0.20209 0.11804 0.4095 ]\n",
      "probability: [0.10488 0.88857 0.98503]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.15636 0.15488 0.15351\n",
      " 0.15223 0.15103 0.14992 0.14887 0.1479  0.14698 0.14611 0.1453  0.14454\n",
      " 0.14382 0.14314 0.14249 0.14188 0.     ]\n",
      "\n",
      "loss 0.08042701574987667\n",
      "J_theta: 0.1413069398175854\n",
      "iter: 50  coef:[0.20216 0.1204  0.41005]\n",
      "probability: [0.10336 0.88926 0.9853 ]\n",
      "loss: [1.01064 0.82831 0.69167 0.58928 0.51166 0.45183 0.40489 0.36743 0.33706\n",
      " 0.31211 0.29136 0.27389 0.25906 0.24635 0.23538 0.22583 0.21748 0.21012\n",
      " 0.20361 0.19782 0.19265 0.18801 0.18383 0.18006 0.17664 0.17353 0.17069\n",
      " 0.1681  0.16572 0.16353 0.16152 0.15967 0.15795 0.15636 0.15488 0.15351\n",
      " 0.15223 0.15103 0.14992 0.14887 0.1479  0.14698 0.14611 0.1453  0.14454\n",
      " 0.14382 0.14314 0.14249 0.14188 0.14131]\n",
      "\n",
      "pred: [1 1]\n",
      "pred_prob: [0.93173 0.63197]\n"
     ]
    }
   ],
   "source": [
    "# Debug data\n",
    "X_train = np.array([[-6, -4], \n",
    "             [2, 4],\n",
    "             [6, 8]])\n",
    "\n",
    "y_train = np.array([0, 1, 1])\n",
    "X_test  = np.array([[3, 5],\n",
    "                    [-4, 2]])\n",
    "\n",
    "\n",
    "lr = ScratchLogisticRegression(num_iter=50, lr=0.01, C=2, verbose=True ,debug=True)\n",
    "lr.fit(X_train, y_train)\n",
    "pred = lr.predict(X_test)\n",
    "pred_prob = lr.predict_proba(X_test)\n",
    "\n",
    "print(\"pred:\", pred)\n",
    "print(\"pred_prob:\", pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5 \n",
    "## Learning and Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn and estimate the scratch implementation for the virgicolor and virginica binary classifications of the iris dataset prepared in Sprint, an introduction to machine learning scratch.\n",
    "\n",
    "Please check if it works properly compared to the implementation by scikit-learn.\n",
    "\n",
    "Use scikit-learn for index values such as Accuracy, Precision, and Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading iris data\n",
    "iris = load_iris()\n",
    "\n",
    "## Select X1 (Petallength) and X2 (petalwidth) as features for learning.\n",
    "train_x = iris.data[50:, 2:]\n",
    "train_y = iris.target[50:]\n",
    "train_y = np.array(list(map(lambda x: x-1, train_y)))\n",
    "\n",
    "## Dividing into training data and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, random_state=42)\n",
    "\n",
    "## Standardization\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch_LR predict: [0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0]\n",
      "sklearn_LR predict: [0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precicion_score</th>\n",
       "      <th>recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scratch_LR</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sklearn_LR</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_name accuracy_score precicion_score recall_score\n",
       "0  scratch_LR           0.88             0.9     0.818182\n",
       "1  sklearn_LR           0.88             0.9     0.818182"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Hyperparameter settings\n",
    "num_iter = 5000\n",
    "alpha = 0.1\n",
    "C = 1.0\n",
    "\n",
    "## Define model\n",
    "scratch_lr = ScratchLogisticRegression(num_iter=num_iter ,lr=alpha, C=C, verbose=False)\n",
    "sklearn_lr = LogisticRegression()\n",
    "\n",
    "model = {\"scratch_LR\": scratch_lr, \"sklearn_LR\": sklearn_lr}\n",
    "results = pd.DataFrame(columns=[\"model_name\", \"accuracy_score\", \"precicion_score\", \"recall_score\"])\n",
    "\n",
    "## Learning and prediction\n",
    "i = 0\n",
    "for name, model in model.items():\n",
    "    \n",
    "    if name == \"scratch_LR\":\n",
    "        model.fit(X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"{name} predict:\", y_pred)\n",
    "    \n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    pre_score = precision_score(y_test, y_pred)\n",
    "    rec_score = recall_score(y_test, y_pred)\n",
    "    \n",
    "    results.loc[i, :] = [name, acc_score, pre_score, rec_score]\n",
    "    i += 1\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the same result was obtained, there seems to be no problem in implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6 \n",
    "\n",
    "## Plot of Learning Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the learning curve to see if the loss is falling properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iter_list = list(range(num_iter))\n",
    "scratch_train_loss = scratch_lr.loss\n",
    "scratch_val_loss = scratch_lr.val_loss\n",
    "\n",
    "plt.scatter(iter_list, scratch_train_loss, label=\"Train_Loss\")\n",
    "plt.scatter(iter_list, scratch_val_loss, label=\"Val_Loss\")\n",
    "plt.xlabel(\"iter\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Model Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7\n",
    "\n",
    "## Visualization of Decision Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "def decision_region(X, y, model, step=0.01, title='Decision Region', xlabel='xlabel', ylabel='ylabel', target_names=['versicolor', 'virginica']):\n",
    "    \"\"\"\n",
    "    Draw the determination area of the model that learned binary classification with two-dimensional features.\n",
    "    The background color is drawn from the estimated values from the trained model.\n",
    "    The points on the scatter plot are training or validation data.\n",
    "\n",
    "    Parameters\n",
    "    ----------------\n",
    "    X : ndarray, shape(n_samples, 2)\n",
    "        Feature value\n",
    "    y : ndarray, shape(n_samples,)\n",
    "        label\n",
    "    model : object\n",
    "        Insert the installed model of the learned model\n",
    "    step : float, (default : 0.1)\n",
    "        Set the interval to calculate the estimate\n",
    "    title : str\n",
    "        Give the text of the graph title\n",
    "    xlabel, ylabel : str\n",
    "        Give the text of the axis label\n",
    "    target_names= : list of str\n",
    "        Give a list of legends\n",
    "    \"\"\"\n",
    "    # setting\n",
    "    scatter_color = ['red', 'blue']\n",
    "    contourf_color = ['pink', 'skyblue']\n",
    "    n_class = 2\n",
    "    # pred\n",
    "    mesh_f0, mesh_f1  = np.meshgrid(np.arange(np.min(X[:,0])-0.5, np.max(X[:,0])+0.5, step), np.arange(np.min(X[:,1])-0.5, np.max(X[:,1])+0.5, step))\n",
    "    mesh = np.c_[np.ravel(mesh_f0),np.ravel(mesh_f1)]\n",
    "    y_pred = model.predict(mesh).reshape(mesh_f0.shape)\n",
    "    # plot\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.contourf(mesh_f0, mesh_f1, y_pred, n_class-1, cmap=ListedColormap(contourf_color))\n",
    "    plt.contour(mesh_f0, mesh_f1, y_pred, n_class-1, colors='y', linewidths=3, alpha=0.5)\n",
    "    for i, target in enumerate(set(y)):\n",
    "        plt.scatter(X[y==target][:, 0], X[y==target][:, 1], s=80, color=scatter_color[i], label=target_names[i], marker='o')\n",
    "    patches = [mpatches.Patch(color=scatter_color[i], label=target_names[i]) for i in range(n_class)]\n",
    "    plt.legend(handles=patches)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "decision_region(X_test, y_test, scratch_lr, xlabel=\"petal length\", ylabel=\"petal width\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 8 \n",
    "\n",
    "## (Advance Task) Saving Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make it possible to save and load the learned weights for easy verification. Use the pickle module and NumPy's np.savez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"model_pickle\", \"wb\") as f:\n",
    "    pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryFile\n",
    "\n",
    "outfile = TemporaryFile()\n",
    "np.savez(outfile, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
