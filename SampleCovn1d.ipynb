{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3189bad",
   "metadata": {},
   "source": [
    "# Implementation of one-dimensional convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b820e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b64378",
   "metadata": {},
   "source": [
    "# First, create appropriate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "848b1f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 784)\n",
      "(10000, 1, 784)\n",
      "uint8\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n",
      "  247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n",
      "  170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n",
      "    0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n",
      "   82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n",
      "  253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n",
      "  225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n",
      "  253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n",
      "   80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 1, 784)\n",
    "X_test = X_test.reshape(-1, 1, 784)\n",
    "\n",
    "print(X_train.shape) ## (60000, 28, 28)\n",
    "print(X_test.shape) ## (10000, 28, 28)\n",
    "print(X_train[0].dtype) ## uint8\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fc72710",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 1, 784)\n",
    "X_test = X_test.reshape(-1, 1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c61ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:2400]\n",
    "y_train = y_train[:2400]\n",
    "X_test = X_test[:600]\n",
    "y_test = y_test[:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fab84a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 0\n",
    "image = X_train[index].reshape(28,28)\n",
    "plt.imshow(image, 'gray')\n",
    "plt.title('label : {}'.format(y_train[index]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c9c479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "(2400,)\n",
      "(2400, 10)\n",
      "float64\n",
      "(1920, 1, 784) (1920, 10)\n",
      "(480, 1, 784) (480, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.max()) ## 1.0\n",
    "print(X_train.min()) ## 0.0\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "print(y_train.shape) ## (60000,)\n",
    "print(y_train_one_hot.shape) ## (60000, 10)\n",
    "print(y_train_one_hot.dtype) ## float64\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "\n",
    "print(X_train.shape, y_train.shape) ## (48000, 784)\n",
    "print(X_val.shape, y_val.shape) ## (12000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0714934",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "## Creation of a one-dimensional convolution layer class limited to one channel number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0486480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scratch1dCNNClassifier:\n",
    "    \"\"\"\n",
    "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
    "     Parameters\n",
    "     ----------\n",
    "     n_nodes1: int\n",
    "       Number of nodes in the previous layer\n",
    "     n_nodes2: int\n",
    "       Number of nodes in the later layer\n",
    "     initializer: instance of initialization method\n",
    "     optimizer: instance of optimization method\n",
    "    \"\"\"\n",
    "    def __init__(self):       \n",
    "        self.W = np.array([3,5,7]) \n",
    "        self.B = np.array([1])\n",
    "        self.padding = 0\n",
    "        self.strides = 1        \n",
    "        self.filters = len(self.W)\n",
    "        self.a = np.array([])\n",
    "        self.dW = np.array([])\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward\n",
    "         Parameters\n",
    "         ----------\n",
    "         X: ndarray, shape (batch_size, n_nodes_bf) of the following form\n",
    "             input\n",
    "         Returns\n",
    "         ----------\n",
    "         A: The following forms of ndarray, shape (batch_size, n_nodes_af)\n",
    "             output\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.N_in = len(self.X)\n",
    "        self.N_out = int((self.N_in + 2*self.padding - self.filters) / self.strides + 1) ## Output size calculation\n",
    "        \n",
    "        self.a = np.append(self.a, [np.dot(self.X[i : i+self.filters], self.W) + self.B for i in range(self.N_out)])\n",
    "\n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward\n",
    "         Parameters\n",
    "         ----------\n",
    "         dA: ndarray, shape (batch_size, n_nodes2) of the following form\n",
    "             Gradient flowing from behind\n",
    "         Returns\n",
    "         ----------\n",
    "         dZ: ndarray, shape (batch_size, n_nodes1) of the following form\n",
    "             Gradient to flow forward\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA, axis=0)        \n",
    "        self.dW = np.append(self.dW, [np.dot(self.X[i : i+self.N_out].T, dA) for i in range(self.filters)])\n",
    "\n",
    "\n",
    "        self.dX = np.zeros(len(self.X))            \n",
    "        for j in range(len(self.X)):\n",
    "            for s in range(len(self.W)):\n",
    "                if j - s < 0 or j - s > 1:\n",
    "                    self.dX[j] = self.dX[j]\n",
    "                else:\n",
    "                    self.dX[j] = self.dX[j] + dA[j - s] * self.W[s]\n",
    "\n",
    "        return self.dX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0d9207",
   "metadata": {},
   "source": [
    "# Problem 2 \n",
    "## Calculation of output size after one-dimensional convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "059a72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_out_shape(N_in, P, F, S):\n",
    "    N_out = ((N_in + 2 * P - F) / S) + 1\n",
    "    return int(N_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ca0359e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "padding=0\n",
    "stride = 1\n",
    "\n",
    "calc_out_shape(4, padding, len(w), stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12277c",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "## Experiment of one-dimensional convolution layer with small array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "136e7da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c07dfd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35., 50.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Scratch1dCNNClassifier()\n",
    "model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a08efc6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30., 110., 170., 140.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_a = np.array([10, 20])\n",
    "model.backward(delta_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419a396",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "## Creation of a one-dimensional convolution layer class that does not limit the number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6b5133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
    "     Parameters\n",
    "     ----------\n",
    "     n_nodes1: int\n",
    "       Number of nodes in the previous layer\n",
    "     n_nodes2: int\n",
    "       Number of nodes in the later layer\n",
    "     initializer: instance of initialization method\n",
    "     optimizer: instance of optimization method\n",
    "    \"\"\"\n",
    "    def __init__(self):       \n",
    "        self.W = np.ones((3, 2, 3)) \n",
    "        self.B = np.array([1, 2, 3])\n",
    "        self.padding = 0\n",
    "        self.strides = 1        \n",
    "        self.filters = self.W.shape[2]        \n",
    "        #self.a = np.array([])\n",
    "        #self.dW = np.array([])\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward\n",
    "         Parameters\n",
    "         ----------\n",
    "         X: ndarray, shape (batch_size, n_nodes_bf) of the following form\n",
    "             input\n",
    "         Returns\n",
    "         ----------\n",
    "         A: The following forms of ndarray, shape (batch_size, n_nodes_af)\n",
    "             output\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.N_in = self.X.shape[1]\n",
    "        self.N_out = int((self.N_in + 2*self.padding - self.filters) / self.strides + 1) ## Output size calculation\n",
    "\n",
    "        self.a = np.zeros((self.W.shape[0], self.N_out)) ## Number of output channels, output size\n",
    "        for i in range(self.W.shape[0]): ## Output channel\n",
    "            for j in range(self.X.shape[0]): ## Input channel\n",
    "                for s in range(self.N_out): ## Output size\n",
    "                    self.a[i, j] = self.a[i, j] + np.dot(self.X[j, s: s+self.filters], self.W[i, j])\n",
    "                    \n",
    "        ## bias\n",
    "        self.a = self.a + self.B.reshape(-1, 1)\n",
    "\n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward\n",
    "         Parameters\n",
    "         ----------\n",
    "         dA: ndarray, shape (batch_size, n_nodes2) of the following form\n",
    "             Gradient flowing from behind\n",
    "         Returns\n",
    "         ----------\n",
    "         dZ: ndarray, shape (batch_size, n_nodes1) of the following form\n",
    "             Gradient to flow forward\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA, axis=1)\n",
    "        \n",
    "        self.dW = np.zeros((self.W.shape[0], self.X.shape[0], self.filters)) ## Number of output channels, output size, filter size\n",
    "        for i in range(self.W.shape[0]): ## Output channel\n",
    "            for j in range(self.X.shape[0]): ## Input channel\n",
    "                for s in range(self.filters): ## Filter size\n",
    "                        self.dW[i, j, s] = np.dot(self.X[j][s : s+self.N_out].T, dA[s])\n",
    "\n",
    "        self.dX = np.zeros(self.X.shape)\n",
    "        for i in range(self.W.shape[0]): ## Output channel\n",
    "            for j in range(self.X.shape[0]): ## Input channel\n",
    "                for s in range(self.filters): ## Filter size\n",
    "                    for n in range(self.N_out): ## Output size\n",
    "                        self.dX[j, s + n] = self.dX[j, s + n] + dA[i, n] * self.W[i, j, s]\n",
    "\n",
    "        return self.dX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f31516",
   "metadata": {},
   "source": [
    "Let's check the output as in problem 3.\n",
    "\n",
    "In forward propagation, when the inputs and parameters are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4b368b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv1d()\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) ## shape (2, 4), (number of input channels, number of features).\n",
    "w = np.ones((3, 2, 3)) ## All are set to 1 for simplification of the example. (Number of output channels, number of input channels, filter size).\n",
    "b = np.array([1, 2, 3]) ## （Number of output channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6adebdd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 22.],\n",
       "       [17., 23.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad9ef845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30., 90., 90., 60.],\n",
       "       [30., 90., 90., 60.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_a = np.array([[10, 20], [10, 20], [10, 20]])\n",
    "model.backward(delta_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c66d6",
   "metadata": {},
   "source": [
    "# Problem 8 \n",
    "## Learning and estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "719a256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    def __init__(self):\n",
    "        self.X_shape = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        ## 1D\n",
    "        X_1d = X.reshape(X.shape[0], -1)\n",
    "        \n",
    "        ## shape record\n",
    "        self.X_shape = X.shape\n",
    "        \n",
    "        return X_1d    \n",
    "\n",
    "    def backward(self, X):\n",
    "        ## Return of shape\n",
    "        X = X.reshape(self.X_shape)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c070c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scratch1dCNNClassifier:\n",
    "    \"\"\"\n",
    "    Deep neural network classifier\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    activaiton: {'sigmoid','tanh','relu'}\n",
    "        Types of activation functions\n",
    "    n_nodes: list\n",
    "        Node configuration example [400, 200, 100]\n",
    "    n_output: int\n",
    "        Number of output layers\n",
    "    alpha: float\n",
    "        Learning rate\n",
    "    optimizer: {'sgd','adagrad'}\n",
    "        Types of optimization methods\n",
    "    filter_num: int\n",
    "        Number of filters\n",
    "    filter_size: int\n",
    "        Filter size\n",
    "        \n",
    "    Attributes\n",
    "    -------------\n",
    "    FC [n_layers]: dict\n",
    "        A dictionary that manages the connection layer\n",
    "    activation: dict\n",
    "        A dictionary that manages the activation function\n",
    "    self.epochs: int\n",
    "        Number of epochs (initial value: 10)\n",
    "    self.batch_size: int\n",
    "        Batch size (initial value: 20)\n",
    "    self.n_features: int\n",
    "        Number of features\n",
    "    self.val_is_true: boolean\n",
    "        Presence or absence of verification data\n",
    "    self.loss: empty ndarray\n",
    "        Record losses on training data\n",
    "    self.loss_val: empty ndarray\n",
    "        Record loss on validation data\n",
    "        \n",
    "    \"\"\"    \n",
    "    def __init__(self, activation, n_nodes, n_output, lr, optimizer, filter_num, filter_size):\n",
    "        self.select_activation = activation\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_output = n_output\n",
    "        self.lr = lr\n",
    "        self.select_optimizer = optimizer\n",
    "        \n",
    "        # Sprint11 addition*******************************\n",
    "        self.filter_num  = filter_num                         #Number of filters\n",
    "        self.filter_size   = filter_size   #Filter size\n",
    "        # Sprint11 added *********************************\n",
    "            \n",
    "    def __initialize_n_layers(self):\n",
    "        \"\"\"\n",
    "       Initialize the N layer.\n",
    "         When the sigmoid function and tanh function are activation functions: Xavier is the initial value\n",
    "         If the ReLU function is an activation function: He is the initial value\n",
    "        \"\"\"\n",
    "        self.activation = dict()\n",
    "        self.FC = dict()\n",
    "        # Sprint11 addition************************************************* \n",
    "        #n_nodes = self.out // (2**(1))\n",
    "        if self.select_activation == 'sigmoid':\n",
    "            self.FC[0] = FC(self.out, self.n_nodes[0], \n",
    "                            XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "            self.activation[0] = Sigmoid()\n",
    "        elif self.select_activation == 'tanh':\n",
    "            self.FC[0] = FC(self.out, self.n_nodes[0], \n",
    "                            XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "            self.activation[0] = Tanh()\n",
    "        elif self.select_activation == 'relu':\n",
    "            self.FC[0] = FC(self.out, self.n_nodes[0], \n",
    "                            HeInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "            self.activation[0] = ReLU()\n",
    "        # Sprint11 addition************************************************* \n",
    "\n",
    "        for n_layer in range(len(self.n_nodes)):            \n",
    "            if n_layer == len(self.n_nodes) -1:\n",
    "                if self.select_activation == 'sigmoid':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_output, \n",
    "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = Softmax()\n",
    "                elif self.select_activation == 'tanh':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_output,\n",
    "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = Softmax()\n",
    "                elif self.select_activation == 'relu':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_output,\n",
    "                                              HeInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = Softmax()\n",
    "            else:\n",
    "                if self.select_activation == 'sigmoid':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_nodes[n_layer+1], \n",
    "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = Sigmoid()\n",
    "                elif self.select_activation == 'tanh':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_nodes[n_layer+1],\n",
    "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = Tanh()\n",
    "                elif self.select_activation == 'relu':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_nodes[n_layer+1],\n",
    "                                              HeInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = ReLU()\n",
    "    \n",
    "    def fit(self, X, y, epochs=10, batch_size=20):  \n",
    "        self.epochs = epochs                            #Epoch number    \n",
    "        self.batch_size = batch_size              #Batch size\n",
    "        self.loss = np.zeros(self.epochs)        # For output of learning curve / objective function (training data)\n",
    "        self.loss_val = np.zeros(self.epochs) # For output of learning curve / objective function (verification data)        \n",
    "        \n",
    "        if self.select_optimizer == 'sgd':\n",
    "            self.optimizer = SGD(self.lr)\n",
    "        elif self.select_optimizer == 'adagrad':\n",
    "            self.optimizer = AdaGrad(self.lr)            \n",
    "        \n",
    "        # Sprint11 added ************************************************* \n",
    "         # Convolution layer class\n",
    "        self.input_channel = X.shape[1]\n",
    "        self.input_feature = X.shape[2]\n",
    "        \n",
    "        self.conv1d = Conv1d(self.select_activation, self.optimizer, self.filter_num, self.input_channel, self.filter_size)\n",
    "        if self.select_activation == 'sigmoid':\n",
    "            self.activation_conv = Sigmoid()\n",
    "        elif self.select_activation == 'tanh':\n",
    "            self.activation_conv = Tanh()\n",
    "        elif self.select_activation == 'relu':\n",
    "            self.activation_conv = ReLU()\n",
    " \n",
    "        #Get mini-batch\n",
    "        get_mini_batch = GetMiniBatch(X, y, self.batch_size)\n",
    "\n",
    "        # Smoothing class\n",
    "        self.flatten = Flatten()\n",
    "        self.out = self.filter_num * (self.input_feature - (self.filter_size - 1))\n",
    "        # Sprint11 addition*************************************************         \n",
    "        \n",
    "        self.__initialize_n_layers()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            #get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size) # Sprint11 Delete\n",
    "            print(\"epoch \", epoch + 1 , \" processing . . .\")\n",
    "            for mini_X_train,  mini_y_train in get_mini_batch:\n",
    "                self.X_ = mini_X_train\n",
    "                self.y_ = mini_y_train\n",
    "                \n",
    "                # Forward propagation\n",
    "                 # Sprint11 added ************************************************* \n",
    "                 # Convolution (1st layer)\n",
    "                self.A = self.conv1d.forward(self.X_) \n",
    "                self.Z = self.activation_conv.forward(self.A)    \n",
    "                \n",
    "                #Smoothing\n",
    "                self.F = self.flatten.forward(self.Z)\n",
    "                               \n",
    "                self.A = self.FC[0].forward(self.F)\n",
    "                self.Z = self.activation[0].forward(self.A)                         \n",
    "                \n",
    "                # Sprint11 addition************************************************* \n",
    "                \n",
    "                self.A = self.FC[1].forward(self.Z)            #1st layer\n",
    "                self.Z = self.activation[1].forward(self.A) #1st layer\n",
    "                for n_layer in range(2, len(self.n_nodes) + 1): #2nd and subsequent layers\n",
    "                    self.A = self.FC[n_layer].forward(self.Z)\n",
    "                    self.Z = self.activation[n_layer].forward(self.A)\n",
    "                \n",
    "                # Backpropagation\n",
    "                self.dA, self.loss[epoch] = self.activation[len(self.n_nodes)].backward(self.Z, self.y_) #Final layer, cross entropy error                self.dZ = self.FC[len(self.n_nodes)].backward(self.dA) #最終層           \n",
    "                for n_layer in reversed(range(0, len(self.n_nodes))): #Final layer -1\n",
    "                    self.dA = self.activation[n_layer].backward(self.dZ)\n",
    "                    self.dZ = self.FC[n_layer].backward(self.dA)\n",
    "\n",
    "                # Sprint11 addition********************************\n",
    "                 # Shape back\n",
    "                self.dF = self.flatten.backward(self.dZ)\n",
    "                \n",
    "                # Convolution layer\n",
    "                self.dA = self.activation_conv.backward(self.dF)\n",
    "                self.dZ = self.conv1d.backward(self.dA)\n",
    "                # Sprint11 addition********************************\n",
    "                \n",
    "    def predict(self,X):\n",
    "        \n",
    "        # Sprint11 addition********************************\n",
    "         # Convolution (1st layer)\n",
    "        self.A = self.conv1d.forward(X) \n",
    "        self.Z = self.activation_conv.forward(self.A)    \n",
    "\n",
    "        #Smoothing\n",
    "        self.F = self.flatten.forward(self.Z)\n",
    "\n",
    "        self.A = self.FC[0].forward(self.F)            # 1 layer\n",
    "        self.Z = self.activation[0].forward(self.A) # 1 layer\n",
    "         # Sprint11 added ********************************\n",
    "        \n",
    "        # Forward propagation\n",
    "        self.A = self.FC[1].forward(self.Z)                    # 1 layer\n",
    "        self.Z = self.activation[1].forward(self.A) # 1 layer\n",
    "        for n_layer in range(2, len(self.n_nodes) + 1): # 2nd layer and beyond\n",
    "            self.A = self.FC[n_layer].forward(self.Z)\n",
    "            self.Z = self.activation[n_layer].forward(self.A)\n",
    "        \n",
    "        return np.argmax(self.Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39fb3576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
    "     Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      Number of nodes in the previous layer\n",
    "    n_nodes2 : int\n",
    "      Number of nodes in the later layer\n",
    "    initializer : Instance of initialization method\n",
    "    optimizer : Instance of optimization method\n",
    "    \"\"\"\n",
    "    def __init__(self, activation, optimizer, filter_num, input_channel, filter_size):\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        self.filter_num = filter_num\n",
    "        self.input_channel = input_channel\n",
    "        self.filter_size = filter_size\n",
    "        self.pad = 0\n",
    "        self.stride = 1\n",
    "        \n",
    "        if self.activation == 'sigmoid':\n",
    "            initializer = XavierInitializer(self.filter_num, self.input_channel, self.filter_size)\n",
    "            self.W = initializer.W(_, _)\n",
    "            self.B = initializer.B(_)            \n",
    "        elif self.activation == 'tanh':\n",
    "            initializer = XavierInitializer(self.filter_num, self.input_channel, self.filter_size)\n",
    "            self.W = initializer.W(_, _)\n",
    "            self.B = initializer.B(_)          \n",
    "        elif self.activation == 'relu':\n",
    "            initializer = HeInitializer(self.filter_num, self.input_channel, self.filter_size)\n",
    "            self.W = initializer.W(_, _)\n",
    "            self.B = initializer.B(_)\n",
    "\n",
    "        self.X = None\n",
    "        self.N_in = None\n",
    "        self.N_out = None\n",
    "        self.a = None\n",
    "        self.dB = None\n",
    "        self.dW = None\n",
    "        self.dX = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following form ndarray, shape (batch_size, n_nodes_bf)\n",
    "            input\n",
    "        Returns\n",
    "        ----------\n",
    "        A : The following form ndarray, shape (batch_size, n_nodes_af)\n",
    "            output\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        \n",
    "        ## Get shape\n",
    "        batch_size, input_channel, self.N_in = self.X.shape \n",
    "        FN, C, FS = self.W.shape \n",
    "        \n",
    "        self.N_out = int((self.N_in + 2*self.pad - self.filter_size) / self.stride + 1) \n",
    "\n",
    "        self.a = np.zeros((batch_size, FN, self.N_out)) \n",
    "        for b in range(batch_size): \n",
    "            for i in range(FN): \n",
    "                for j in range(C): \n",
    "                    for s in range(self.N_out): \n",
    "                        self.a[b, i, s] = self.a[b, i, s] + np.sum(self.X[b, j, s: s+self.filter_size] * self.W[i, j, :])\n",
    "                        \n",
    "       ## bias\n",
    "        self.a = self.a + self.B.reshape(1, -1, 1)\n",
    "\n",
    "        return self.a\n",
    "       \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : The following form ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient flowing from behind\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : The following form ndarray, shape (batch_size, n_nodes1)\n",
    "            Gradient to flow forward\n",
    "        \"\"\"\n",
    "        batch, C, W = self.X.shape \n",
    "        FN, C, FS = self.W.shape \n",
    "        \n",
    "        ## Empty array\n",
    "        self.dW = np.zeros(self.W.shape) \n",
    "        self.dX = np.zeros(self.X.shape) \n",
    "\n",
    "        self.dB = np.sum(dA, axis=2)\n",
    "        \n",
    "        for b in range(batch): \n",
    "            for i in range(FN): \n",
    "                for j in range(C):\n",
    "                    for s in range(FS): \n",
    "                        for x in range(self.N_out): \n",
    "                            self.dW[i, j, s] = self.dW[i, j, s] + dA[b, i, x] * self.X[b, j, s + x]\n",
    "                            self.dX[b, j, s + x] = self.dX[b, j, s + x] + dA[b, i, x] * self.W[i, j, s]\n",
    "\n",
    "        ## Update\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return self.dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c013a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "     Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "     Number of nodes in the previous layer\n",
    "    n_nodes2 : int\n",
    "      Number of nodes in the later layer\n",
    "    initializer : Instance of initialization method\n",
    "    optimizer : Instance of optimization method\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        ## Initialize\n",
    "        \n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward\n",
    "         Parameters\n",
    "         ----------\n",
    "         X: ndarray, shape (batch_size, n_nodes1) of the following form\n",
    "             input\n",
    "         Returns\n",
    "         ----------\n",
    "         A: ndarray, shape (batch_size, n_nodes2) of the following form\n",
    "             output\n",
    "        \"\"\"        \n",
    "        self.X = X\n",
    "        \n",
    "        return np.dot(self.X, self.W) + self.B\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward\n",
    "         Parameters\n",
    "         ----------\n",
    "         dA: ndarray, shape (batch_size, n_nodes2) of the following form\n",
    "             Gradient flowing from behind\n",
    "         Returns\n",
    "         ----------\n",
    "         dZ: ndarray, shape (batch_size, n_nodes1) of the following form\n",
    "             Gradient to flow forward\n",
    "        \"\"\"\n",
    "        self.dB = dA\n",
    "        self.dW = np.dot(self.X.T, dA)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "\n",
    "        ## Update\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06c39bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavier class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filter_num=None, input_channel=None, filter_size=None):        \n",
    "        self.filter_num = filter_num\n",
    "        self.input_channel = input_channel\n",
    "        self.filter_size = filter_size\n",
    "     \n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "         Parameters\n",
    "         ----------\n",
    "         n_nodes1: int\n",
    "           Number of nodes in the previous layer\n",
    "         n_nodes2: int\n",
    "           Number of nodes in the later layer\n",
    "\n",
    "         Returns\n",
    "         ----------\n",
    "         W: ndarray, shape (n_nodes1, n_nodes2) of the following form\n",
    "             weight\n",
    "        \"\"\"\n",
    "                \n",
    "        if self.filter_num and self.input_channel and self.filter_size is not None: \n",
    "            W = np.random.randn(self.filter_num, self.input_channel, self.filter_size)\n",
    "        else: \n",
    "        \n",
    "            W = np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)       \n",
    "        \n",
    "        return W\n",
    "        \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "         Parameters\n",
    "         ----------\n",
    "         n_nodes2: int\n",
    "           Number of nodes in the later layer\n",
    "\n",
    "         Returns\n",
    "         ----------\n",
    "         B: ndarray, shape (n_nodes2,) of the following form\n",
    "             bias\n",
    "        \"\"\"\n",
    "                \n",
    "        if self.filter_num and self.input_channel and self.filter_size is not None: \n",
    "            B = np.random.randn(self.filter_num)\n",
    "        else: \n",
    "        \n",
    "            B = np.zeros(n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d790519",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    He class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filter_num=None, input_channel=None, filter_size=None):        \n",
    "        self.filter_num = filter_num\n",
    "        self.input_channel = input_channel\n",
    "        self.filter_size = filter_size\n",
    "           \n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "       Weight initialization\n",
    "         Parameters\n",
    "         ----------\n",
    "         n_nodes1: int\n",
    "           Number of nodes in the previous layer\n",
    "         n_nodes2: int\n",
    "           Number of nodes in the later layer\n",
    "\n",
    "         Returns\n",
    "         ----------\n",
    "         W: ndarray, shape (n_nodes1, n_nodes2) of the following form\n",
    "             weight\n",
    "        \"\"\"\n",
    "               \n",
    "        if self.filter_num and self.input_channel and self.filter_size is not None: \n",
    "            W = np.random.randn(self.filter_num, self.input_channel, self.filter_size) * np.sqrt(2 / self.filter_num) \n",
    "        else: \n",
    "        \n",
    "            W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1)\n",
    "    \n",
    "        return W\n",
    "        \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "         Parameters\n",
    "         ----------\n",
    "         n_nodes2: int\n",
    "           Number of nodes in the later layer\n",
    "\n",
    "         Returns\n",
    "         ----------\n",
    "         B: ndarray, shape (n_nodes2,) of the following form\n",
    "             bias\n",
    "        \"\"\"\n",
    "              \n",
    "        if self.filter_num and self.input_channel and self.filter_size is not None: \n",
    "            B = np.random.randn(self.filter_num)\n",
    "        else: \n",
    "        \n",
    "            B = np.random.randn(n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21389183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "Stochastic gradient descent\n",
    "     Parameters\n",
    "     ----------\n",
    "     lr: Learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "       Update weights and biases for a layer\n",
    "         Parameters\n",
    "         ----------\n",
    "         layer: Instance of the layer before update\n",
    "        \n",
    "         Returns\n",
    "         ----------\n",
    "         layer: Updated layer instance\n",
    "        \"\"\"\n",
    "        layer.W = layer.W - self.lr * layer.dW\n",
    "        layer.B = layer.B - self.lr * layer.dB.mean(axis=0)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fbeaade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGrad class\n",
    "     Parameters\n",
    "     ----------\n",
    "     alpha: Learning rate\n",
    "    \n",
    "     Attributes\n",
    "     -------------\n",
    "     lr: Learning rate\n",
    "     HW: int (initial value), ndarray\n",
    "     HB: int (initial value), ndarray\n",
    "    \"\"\"    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.HW= 0 \n",
    "        self.HB = 0       \n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases for a layer\n",
    "         Parameters\n",
    "         ----------\n",
    "         layer: instance\n",
    "             Instance of the layer before update\n",
    "\n",
    "         Returns\n",
    "         ----------\n",
    "         layer: instance\n",
    "             Updated tier instance\n",
    "        \"\"\"\n",
    "        ## Initialize\n",
    "        self.HW = np.zeros_like(layer.W)\n",
    "        self.HB = np.zeros_like(layer.B)\n",
    "        \n",
    "       ## update\n",
    "        self.HW = self.HW + (layer.dW**2) #/ layer.dB.shape[0]\n",
    "        self.HB = self.HB + (layer.dB**2).mean(axis=0)\n",
    "        layer.W = layer.W - self.lr * 1 / np.sqrt(self.HW + 1e-7) * layer.dW #/ layer.dB.shape[0]\n",
    "        layer.B = layer.B - self.lr * 1 / np.sqrt(self.HB + 1e-7) * layer.dB.mean(axis=0)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f331989",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        \n",
    "        self.Z = 1.0 / (1.0 + np.exp(-self.A))\n",
    "        \n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "\n",
    "        return dZ * (1 - self.Z) * self.Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aabc2dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        \n",
    "        return np.tanh(self.A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \n",
    "        return dZ * (1.0 - (np.tanh(self.A) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f4f27f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, A):    \n",
    "        self.A = A\n",
    "        return np.maximum(self.A, 0)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return np.where(self.A > 0, dZ, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92f9270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \n",
    "        return np.exp(A) / np.sum(np.exp(A), axis=1, keepdims=True)\n",
    "    \n",
    "    def backward(self, Z, y):\n",
    "        \n",
    "        dA = Z - y\n",
    "        \n",
    "        ## Cross entropy error\n",
    "        batch_size = y.shape[0]\n",
    "        loss = -np.sum(y * np.log(Z)) / batch_size\n",
    "\n",
    "        return dA, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Scratch1dCNNClassifier(activation='tanh', n_nodes=[400, 200, 100], n_output=10, lr=0.001, optimizer='sgd', filter_num=3, filter_size=3)\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=10)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffd2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy : {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0941ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8d404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
